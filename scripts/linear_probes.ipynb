{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371c087b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.281343Z",
     "start_time": "2025-09-05T15:31:40.523387Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PaliGemmaForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# from random import random\n",
    "import random\n",
    "def seed_everywhere(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)    \n",
    "SEED = 42\n",
    "MAX_SEQ_LEN = 64 # location to truncate our inputs\n",
    "DEVICE_1 = 'cuda:1'\n",
    "DEVICE_2 = 'cuda:2' # the second GPU\n",
    "NUM_CLASSES = 0\n",
    "seed_everywhere(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec60d31",
   "metadata": {},
   "source": [
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215fe438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.311567Z",
     "start_time": "2025-09-05T15:31:46.286374Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActivationDataset(Dataset):\n",
    "    \"\"\"Dataset for model activations with labels\"\"\"\n",
    "    def __init__(self, activations, labels):\n",
    "        self.activations = torch.tensor(activations, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.activations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.activations[idx], self.labels[idx]\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"Simple linear probe for classification\"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LinearProbingExperiment:\n",
    "    def __init__(self, model_name=\"gemma\", concept=\"animals\"):\n",
    "        self.model_name = model_name\n",
    "        self.concept = concept\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     def load_activations(self, file_path):\n",
    "#         \"\"\"Load activations from file (assuming numpy format)\"\"\"\n",
    "#         data = np.load(file_path, allow_pickle=True)\n",
    "#         return data['activations'], data['labels']\n",
    "    \n",
    "    def train_sklearn_probe(self,\n",
    "                             X_train: np.ndarray,\n",
    "                             y_train: np.ndarray,\n",
    "                             X_test: np.ndarray,\n",
    "                             y_test: np.ndarray,\n",
    "                             texts_test: list = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Train a sklearn logistic regression probe on flattened activations,\n",
    "        and return the trained probe and a results dict including misclassified samples:\n",
    "        - train_acc, test_acc, classification_report\n",
    "        - y_test, y_pred\n",
    "        - misclassified: list of dicts with index, text, true_label, pred_label\n",
    "        \"\"\"\n",
    "        # Set random state for reproducibility\n",
    "        probe = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "\n",
    "        # Flatten sequence and hidden dimensions\n",
    "        n_train_samples = X_train.shape[0]\n",
    "        n_test_samples = X_test.shape[0]\n",
    "        X_train_flat = X_train.reshape(n_train_samples, -1)\n",
    "        X_test_flat = X_test.reshape(n_test_samples, -1)\n",
    "        print(f\"X_train_flat shape: {X_train_flat.shape}\")\n",
    "        print(f\"y_train shape: {len(y_train)}\")\n",
    "        print(f\"X_test_flat shape: {X_test_flat.shape}\")\n",
    "        print(f\"y_test shape: {len(y_test)}\")\n",
    "\n",
    "        # Fit probe\n",
    "        probe.fit(X_train_flat, y_train)\n",
    "\n",
    "        # Predict\n",
    "        train_pred = probe.predict(X_train_flat)\n",
    "        test_pred = probe.predict(X_test_flat)\n",
    "\n",
    "        # Compute metrics\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        report = classification_report(y_test, test_pred)\n",
    "\n",
    "        # Build results dict\n",
    "        results = {\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'classification_report': report,\n",
    "            'y_test': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "            'y_pred': test_pred.tolist(),\n",
    "        }\n",
    "\n",
    "        # Collect misclassified samples\n",
    "        misclassified = []\n",
    "        if texts_test is not None:\n",
    "            for idx, (true_label, pred_label) in enumerate(zip(y_test, test_pred)):\n",
    "                if true_label != pred_label:\n",
    "                    misclassified.append({\n",
    "                        'index': idx,\n",
    "                        'text': texts_test[idx],\n",
    "                        'true_label': int(true_label),\n",
    "                        'pred_label': int(pred_label)\n",
    "                    })\n",
    "        results['misclassified'] = misclassified\n",
    "\n",
    "        return probe, results\n",
    "    \n",
    "    def train_torch_probe(self, X_train, y_train, X_test, y_test, epochs=100):\n",
    "        \"\"\"Train PyTorch linear probe\"\"\"\n",
    "        input_dim = X_train.shape[0] # is this \"how many text snppets\"?\n",
    "#         num_classes = len(np.unique(y_train)) \n",
    "        num_classes = NUM_CLASSES # let's do binary classifier\n",
    "    # TODO: let's imitate sigmoid\n",
    "        train_dataset = ActivationDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        probe = LinearProbe(input_dim, num_classes).to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(probe.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            probe.train()\n",
    "            total_loss = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = probe(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "            test_outputs = probe(X_test_tensor)\n",
    "            test_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "            train_outputs = probe(X_train_tensor)\n",
    "            train_pred = torch.argmax(train_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        results = {\n",
    "            'train_acc': accuracy_score(y_train, train_pred),\n",
    "            'test_acc': accuracy_score(y_test, test_pred),\n",
    "            'classification_report': classification_report(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        return probe, results\n",
    "    \n",
    "    def run_experiment(self, gemma_activations, gemma_labels, \n",
    "                      polygemma_activations, polygemma_labels):\n",
    "        \"\"\"Run complete probing experiment\"\"\"\n",
    "        print(f\"Running linear probing experiment: {self.concept}\")\n",
    "        print(f\"Gemma training data: {gemma_activations.shape}\")\n",
    "        print(f\"PolyGemma test data: {polygemma_activations.shape}\")\n",
    "        \n",
    "        # Train on Gemma, test on PolyGemma\n",
    "        results = {}\n",
    "    \n",
    "        \n",
    "        # sklearn probe\n",
    "        print(\"\\n--- Training sklearn probe ---\")\n",
    "        # Do the split!\n",
    "        gemma_activations_train, _, gemma_labels_train, _  = train_test_split(gemma_activations, gemma_labels, test_size=0.2, random_state=SEED)\n",
    "        _, polygemma_activations_test, _, polygemma_labels_test = train_test_split(polygemma_activations, polygemma_labels, test_size=0.2, random_state=SEED)\n",
    "        print(f\"size of gemma_activations_train: {gemma_activations_train.shape}, size of gemma_labels_train: {len(gemma_labels_train)}\")\n",
    "        print(f\"size of polygemma_activations_test: {polygemma_activations_test.shape}, size of polygemma_labels_test: {len(polygemma_labels_test)}\")\n",
    "        try:\n",
    "            gemma_activations_train = gemma_activations_train.cpu()\n",
    "            polygemma_activations_test = polygemma_activations_test.cpu()\n",
    "        except:\n",
    "            pass\n",
    "        sklearn_probe, sklearn_results = self.train_sklearn_probe(\n",
    "            gemma_activations_train, gemma_labels_train, # train\n",
    "            polygemma_activations_test, polygemma_labels_test # test\n",
    "        )\n",
    "        results['sklearn'] = sklearn_results\n",
    "        \n",
    "#         # PyTorch probe\n",
    "#         print(\"\\n--- Training PyTorch probe ---\")\n",
    "#         torch_probe, torch_results = self.train_torch_probe(\n",
    "#             gemma_activations, gemma_labels, # train\n",
    "#             polygemma_activations, polygemma_labels # test\n",
    "#         )\n",
    "#         results['torch'] = torch_results\n",
    "        \n",
    "        return sklearn_probe, results\n",
    "    \n",
    "    def save_results(self, results, output_path):\n",
    "        \"\"\"Save experiment results\"\"\"\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        def convert_numpy(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        json_results = json.loads(json.dumps(results, default=convert_numpy))\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0af53",
   "metadata": {},
   "source": [
    "# synth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0d0bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.321968Z",
     "start_time": "2025-09-05T15:31:46.315278Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Synthetic text data for cat/dog classification\n",
    "# text = [\n",
    "#     # Cat examples (label 0)\n",
    "#     \"The fluffy cat purred softly on the windowsill, watching birds outside.\",\n",
    "#     \"My kitten loves to chase the red laser pointer around the living room.\",\n",
    "#     \"The orange tabby cat stretched lazily in the warm afternoon sunlight.\",\n",
    "#     \"She adopted a rescue cat from the local animal shelter last week.\",\n",
    "#     \"The cat's whiskers twitched as it stalked the toy mouse across the floor.\",\n",
    "#     \"Fluffy meowed loudly when her food bowl was empty this morning.\",\n",
    "#     \"The black cat gracefully jumped onto the kitchen counter with ease.\",\n",
    "#     \"My feline friend enjoys napping in cardboard boxes all day long.\",\n",
    "#     \"The cat's green eyes glowed mysteriously in the dim moonlight tonight.\",\n",
    "#     \"Her pet cat brings dead mice to the doorstep every morning.\",\n",
    "#     \"The Siamese cat has the most beautiful blue eyes I've ever seen.\",\n",
    "#     \"Tom cat climbed up the tall oak tree to escape the neighborhood dogs.\",\n",
    "#     \"The veterinarian said the kitten needs its vaccinations next month.\",\n",
    "#     \"My cat purrs so loudly it sounds like a tiny motor running.\",\n",
    "#     \"The calico cat had three adorable kittens in the barn yesterday.\",\n",
    "#     \"She trained her cat to use the toilet instead of a litter box.\",\n",
    "#     \"The Persian cat's long fur requires daily brushing to prevent matting.\",\n",
    "#     \"My indoor cat watches wildlife documentaries on TV with great interest.\",\n",
    "#     \"The stray cat finally trusted me enough to eat from my hand.\",\n",
    "#     \"Her cat knocked over the expensive vase while chasing a butterfly.\",\n",
    "    \n",
    "#     # Dog examples (label 1)\n",
    "#     \"The golden retriever barked excitedly when his owner came home today.\",\n",
    "#     \"My dog loves to fetch tennis balls in the backyard every afternoon.\",\n",
    "#     \"The small puppy wagged its tail when meeting new people yesterday.\",\n",
    "#     \"She takes her German shepherd for long walks in the park.\",\n",
    "#     \"The dog's tail wagged furiously when it saw the treat jar.\",\n",
    "#     \"Max barked at the mailman who comes by every morning.\",\n",
    "#     \"The border collie herded the sheep expertly across the green field.\",\n",
    "#     \"My canine companion loves swimming in the lake during hot summers.\",\n",
    "#     \"The dog trainer taught the puppy basic commands like sit and stay.\",\n",
    "#     \"Her loyal dog waited patiently outside the grocery store for her.\",\n",
    "#     \"The beagle's nose led it straight to the hidden treats upstairs.\",\n",
    "#     \"My dog howls along with the sirens from passing fire trucks.\",\n",
    "#     \"The veterinarian recommended a special diet for the overweight bulldog.\",\n",
    "#     \"The rescue dog was nervous but gradually warmed up to us.\",\n",
    "#     \"My puppy chewed up my favorite pair of running shoes yesterday.\",\n",
    "#     \"The dog park was crowded with excited pups playing together today.\",\n",
    "#     \"Her service dog helps her navigate safely through busy city streets.\",\n",
    "#     \"The hunting dog pointed steadily at the birds hiding in bushes.\",\n",
    "#     \"My dog greets every visitor with enthusiastic tail wagging and jumping.\",\n",
    "#     \"The old dog slept peacefully by the fireplace on cold nights.\",\n",
    "    \n",
    "#     # Neutral/other examples (label 2) - neither cats nor dogs\n",
    "#     \"The morning sun cast beautiful shadows across the empty parking lot.\",\n",
    "#     \"She enjoyed reading mystery novels while drinking her evening tea.\",\n",
    "#     \"The mathematics professor explained complex equations on the whiteboard clearly.\",\n",
    "#     \"Fresh vegetables from the farmers market made an excellent dinner tonight.\",\n",
    "#     \"The old library contained thousands of books on various subjects.\",\n",
    "#     \"He repaired the broken bicycle tire using tools from the garage.\",\n",
    "#     \"The weather forecast predicted rain for the entire weekend ahead.\",\n",
    "#     \"Students gathered in the cafeteria to discuss their upcoming project.\",\n",
    "#     \"The concert featured amazing performances by local musicians and bands.\",\n",
    "#     \"She planted colorful flowers in her garden beds this spring.\",\n",
    "#     \"The computer program crashed unexpectedly during the important presentation today.\",\n",
    "#     \"Ocean waves crashed against the rocky cliffs during the storm.\",\n",
    "#     \"The chef prepared an elaborate feast for the wedding celebration.\",\n",
    "#     \"Mountains covered in snow looked majestic against the clear sky.\",\n",
    "#     \"The museum displayed artifacts from ancient civilizations throughout history.\",\n",
    "#     \"Traffic was heavy on the highway during rush hour yesterday.\",\n",
    "#     \"The smartphone battery died right before the important phone call.\",\n",
    "#     \"Autumn leaves fell gently from the trees in vibrant colors.\",\n",
    "#     \"The construction workers finished building the new bridge ahead of schedule.\",\n",
    "#     \"She studied diligently for her final exams in the quiet library.\"\n",
    "# ]\n",
    "\n",
    "# # Corresponding labels: 0=cat, 1=dog, 2=neutral\n",
    "# labels = [\n",
    "#     # Cat labels (0)\n",
    "#     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#     # Dog labels (1) \n",
    "#     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#     # Neutral labels (2)\n",
    "#     2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
    "# ]\n",
    "\n",
    "# # Verify data consistency\n",
    "# print(f\"Total texts: {len(text)}\")\n",
    "# print(f\"Total labels: {len(labels)}\")\n",
    "# print(f\"Cat examples: {labels.count(0)}\")\n",
    "# print(f\"Dog examples: {labels.count(1)}\")\n",
    "# print(f\"Neutral examples: {labels.count(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626a9bd",
   "metadata": {},
   "source": [
    "# COCO annotations & images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf7486a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.373737Z",
     "start_time": "2025-09-05T15:31:46.324447Z"
    }
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = '../data'\n",
    "\n",
    "cat_df = pd.read_csv(f'{SAVE_DIR}/coco_val2017_cat_binary_with_captions_balanced.csv')\n",
    "cat_df = cat_df.sample(n=min(len(cat_df), 1000), random_state=SEED)\n",
    "\n",
    "dog_df = pd.read_csv(f'{SAVE_DIR}/coco_val2017_dog_binary_with_captions_balanced.csv')\n",
    "dog_df = dog_df.sample(n=min(len(dog_df), 1000), random_state=SEED)\n",
    "\n",
    "human_df = pd.read_csv(f'{SAVE_DIR}/coco_val2017_human_binary_with_captions_balanced.csv')\n",
    "human_df = human_df.sample(n=min(len(human_df), 1000), random_state=SEED)\n",
    "\n",
    "cat_df.captions = cat_df.captions.apply(lambda x:eval(x)[0])\n",
    "dog_df.captions = dog_df.captions.apply(lambda x:eval(x)[0])\n",
    "human_df.captions = human_df.captions.apply(lambda x:eval(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c62c6cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.385473Z",
     "start_time": "2025-09-05T15:31:46.375221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     image_id         file_name  \\\n",
       " 165     42888  000000042888.jpg   \n",
       " 33     532493  000000532493.jpg   \n",
       " 15     153299  000000153299.jpg   \n",
       " 312    284623  000000284623.jpg   \n",
       " 57     329319  000000329319.jpg   \n",
       " ..        ...               ...   \n",
       " 71     492282  000000492282.jpg   \n",
       " 106      2153  000000002153.jpg   \n",
       " 270    364297  000000364297.jpg   \n",
       " 348      6894  000000006894.jpg   \n",
       " 102    455219  000000455219.jpg   \n",
       " \n",
       "                                               captions  label  \n",
       " 165  A corner street sign with a tow sign and a art...      0  \n",
       " 33      A surfer on a white board riding a small wave.      0  \n",
       " 15   two giraffes are standing together outside a barn      0  \n",
       " 312               A black cat sits in a bathroom sink.      1  \n",
       " 57   A black and white cat sitting on top of a wood...      1  \n",
       " ..                                                 ...    ...  \n",
       " 71    A man who is riding a horse down a brick street.      0  \n",
       " 106  Batter preparing to swing at pitch during majo...      0  \n",
       " 270          There is a cat laying down on a keyboard.      1  \n",
       " 348  A man getting a kiss on the neck from an eleph...      0  \n",
       " 102  Some people in a field working with some big a...      0  \n",
       " \n",
       " [368 rows x 4 columns],\n",
       " label\n",
       " 0    184\n",
       " 1    184\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df, cat_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c296c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.391790Z",
     "start_time": "2025-09-05T15:31:46.387175Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os, io, concurrent.futures\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "\n",
    "# def vision_feats_from_bytes(byte_list):\n",
    "#     imgs = [Image.open(io.BytesIO(b)).convert(\"RGB\") for b in byte_list]\n",
    "#     batch = processor(images=imgs, return_tensors=\"pt\")\n",
    "#     batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         vout = model.vision_tower(\n",
    "#             pixel_values=batch[\"pixel_values\"],\n",
    "#             output_hidden_states=True, return_dict=True\n",
    "#         )\n",
    "\n",
    "#     # Choose a layer/aggregation for your probe:\n",
    "#     # - CLS token if the encoder has one (often index 0)\n",
    "#     # - Mean pool over patch tokens\n",
    "#     # Many people use the penultimate layer for probes:\n",
    "#     last_hidden = vout.hidden_states[-2]   # (B, N_tokens, D)\n",
    "#     # If there is a CLS token: feats = last_hidden[:, 0]\n",
    "#     feats = last_hidden.mean(dim=1)        # mean pool over tokens -> (B, D)\n",
    "#     return feats  # detach if you like: feats.detach()\n",
    "\n",
    "# def _read_bytes(path: str) -> bytes:\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def preload_image_bytes(df: pd.DataFrame, root: str, max_workers: int = 8) -> dict:\n",
    "#     paths = [os.path.join(root, fn) for fn in df[\"file_name\"]]\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "#         data = list(ex.map(_read_bytes, paths))\n",
    "#     # map by file_name (or image_id if you prefer)\n",
    "#     return dict(zip(df[\"file_name\"], data))\n",
    "\n",
    "# data_name = 'cat'\n",
    "# ROOT = f\"../data/coco_val2017_{data_name}_binary_with_captions_balanced_images\"  # where 000000323303.jpg etc. live\n",
    "\n",
    "# # ---- load to RAM (compressed) ----\n",
    "# byte_cache = preload_image_bytes(cat_df, ROOT, max_workers=12)\n",
    "\n",
    "# # ---- PaliGemma setup ----\n",
    "# model_id = \"google/paligemma2-3b-pt-224\"\n",
    "# dtype = torch.float32\n",
    "# model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "#     model_id, torch_dtype=dtype, device_map=\"auto\"\n",
    "# )\n",
    "# model.vision_tower.config.output_hidden_states = True\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# # ---- build a batch from in-RAM bytes and run forward/generate ----\n",
    "# def make_batch_from_bytes(filenames, prompts):\n",
    "#     imgs = [Image.open(io.BytesIO(byte_cache[fn])).convert(\"RGB\") for fn in filenames]\n",
    "#     # Processor handles resizing/normalization\n",
    "#     inputs = processor(images=imgs, text=prompts, return_tensors=\"pt\", padding=True)\n",
    "#     return {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# # Example: yes/no “cat” probe for a subset\n",
    "# filenames = cat_df[\"file_name\"].head(16).tolist()\n",
    "# prompts = [\"Is there a cat in this image? Answer yes or no.\"] * len(filenames)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     batch = make_batch_from_bytes(filenames, prompts)\n",
    "#     out = model.generate(**batch, max_new_tokens=5)\n",
    "\n",
    "# preds = processor.batch_decode(out, skip_special_tokens=True)\n",
    "# print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7acc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3b6553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.399576Z",
     "start_time": "2025-09-05T15:31:46.393366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165    52\n",
       "33     46\n",
       "15     49\n",
       "312    36\n",
       "57     55\n",
       "       ..\n",
       "71     48\n",
       "106    53\n",
       "270    41\n",
       "348    57\n",
       "102    53\n",
       "Name: captions, Length: 368, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df.captions.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249cc319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.404221Z",
     "start_time": "2025-09-05T15:31:46.401090Z"
    }
   },
   "outputs": [],
   "source": [
    "texts_list, labels_list, filenames_list = [], [], [] # We don;t save imgs deitecely in RAM since that's not efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aec6cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.412095Z",
     "start_time": "2025-09-05T15:31:46.405835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of cat_texts: 368, length of cat_labels: 368\n",
      "length of cat_texts: 368, length of cat_labels: 368\n",
      "length of human_texts: 1000, length of human_labels: 1000\n"
     ]
    }
   ],
   "source": [
    "cat_texts, cat_labels, cat_filenames = cat_df.captions.tolist(), cat_df.label.tolist(), cat_df.file_name.tolist()\n",
    "texts_list.append(cat_texts), labels_list.append(cat_labels), filenames_list.append(cat_filenames)\n",
    "print(f\"length of cat_texts: {len(cat_texts)}, length of cat_labels: {len(cat_labels)}\")\n",
    "dog_texts, dog_labels, dog_filenames = dog_df.captions.tolist(), dog_df.label.tolist(), dog_df.file_name.tolist()\n",
    "texts_list.append(dog_texts), labels_list.append(dog_labels), filenames_list.append(dog_filenames)\n",
    "print(f\"length of cat_texts: {len(cat_texts)}, length of cat_labels: {len(cat_labels)}\")\n",
    "human_texts, human_labels, human_filenames = human_df.captions.tolist(), human_df.label.tolist(), human_df.file_name.tolist()\n",
    "texts_list.append(human_texts), labels_list.append(human_labels), filenames_list.append(human_filenames)\n",
    "print(f\"length of human_texts: {len(human_texts)}, length of human_labels: {len(human_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe78ef5",
   "metadata": {},
   "source": [
    "# load model & get act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f377208e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:46.425774Z",
     "start_time": "2025-09-05T15:31:46.415133Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_models_with_eval(model_name, device=\"cuda\", return_with_vision_tower = False):\n",
    "    if \"paligemma\" in model_name.lower():\n",
    "        from transformers import PaliGemmaForConditionalGeneration\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32,  # Use fp16 for memory efficiency\n",
    "            device_map=None  # We'll handle device placement manually\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        language_model = model.language_model\n",
    "        if return_with_vision_tower:\n",
    "            return model\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=None\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        language_model = model\n",
    "            \n",
    "    language_model.eval()\n",
    "    return language_model\n",
    "\n",
    "def get_inputs_from_text(model_name, text, device):\n",
    "#     if \"pali\" not in model_name:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"google/paligemma2-3b-pt-224\", use_fast=True) # Let's use paligemma tokeniser to avoid BOS token (same as in SAEs)\n",
    "#     else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # FIXED: More robust tokenization with proper padding token handling\n",
    "    # Ensure we have a pad token\n",
    "\n",
    "    # Tokenize with safer parameters\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        add_special_tokens=True  # Ensure special tokens are added properly\n",
    "    )\n",
    "    if tokenizer.bos_token_id is not None:\n",
    "        inputs[\"attention_mask\"][:, 0] *= (inputs[\"input_ids\"][:, 0] != tokenizer.bos_token_id).to(inputs[\"attention_mask\"].dtype)\n",
    "    # FIXED: Validate token IDs are within vocabulary range\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Check for out-of-bounds token IDs\n",
    "    if torch.any(input_ids >= vocab_size) or torch.any(input_ids < 0):\n",
    "        print(f\"⚠️  Invalid token IDs detected. Max ID: {input_ids.max()}, Vocab size: {vocab_size}\")\n",
    "        # Clamp invalid IDs to valid range\n",
    "        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
    "        inputs['input_ids'] = input_ids\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    return inputs\n",
    "    \n",
    "def get_acts(language_model, text, model_name, DEVICE):\n",
    "    inputs = get_inputs_from_text(model_name, text, DEVICE)\n",
    "    if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "        if layer < len(language_model.model.layers):\n",
    "            target_layer = language_model.model.layers[layer]\n",
    "        else:\n",
    "            print(f\"❌ Layer {layer} out of range. Model has {len(language_model.model.layers)} layers\")\n",
    "            return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "        \n",
    "    activations = language_model(**inputs, output_hidden_states=True)['hidden_states']\n",
    "    return activations\n",
    "#     activations = None\n",
    "\n",
    "\n",
    "#     def activation_hook(module, inputs, output):\n",
    "#         nonlocal activations\n",
    "#         try:\n",
    "#             if isinstance(output, tuple):\n",
    "#                 activations = output[0].clone().detach()\n",
    "#             else:\n",
    "#                 activations = output.clone().detach()\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️  Error in activation hook: {e}\")\n",
    "\n",
    "#     # FIXED: More robust layer identification\n",
    "#     target_layer = None\n",
    "#     try:\n",
    "#         if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "#             if layer < len(language_model.model.layers):\n",
    "#                 target_layer = language_model.model.layers[layer]\n",
    "#             else:\n",
    "#                 print(f\"❌ Layer {layer} out of range. Model has {len(language_model.model.layers)} layers\")\n",
    "#                 return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "#         elif hasattr(language_model, 'layers'):\n",
    "#             if layer < len(language_model.layers):\n",
    "#                 target_layer = language_model.layers[layer]\n",
    "#             else:\n",
    "#                 print(f\"❌ Layer {layer} out of range. Model has {len(language_model.layers)} layers\")\n",
    "#                 return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "#         else:\n",
    "#             print(f\"❌ Could not find layers in model structure\")\n",
    "#             return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error accessing layer {layer}: {e}\")\n",
    "#         return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "\n",
    "#     if target_layer is None:\n",
    "#         print(f\"❌ Could not find layer {layer}\")\n",
    "#         return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "\n",
    "#     hook = target_layer.register_forward_hook(activation_hook)\n",
    "\n",
    "#     # Forward pass to get activations\n",
    "#     with torch.no_grad():\n",
    "#         try:\n",
    "#             if \"paligemma\" in model_name.lower():\n",
    "#                 _ = language_model(**inputs)\n",
    "#             else:\n",
    "#                 _ = language_model(**inputs)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️  Error in activation extraction: {e}\")\n",
    "\n",
    "#     hook.remove()\n",
    "#     if activations is None:\n",
    "#         print(f\"⚠️  Failed to extract activations from layer {layer}\")\n",
    "#         # FIXED: Return appropriate tensor size based on model\n",
    "#         try:\n",
    "#             # Try to get the actual hidden size from the model config\n",
    "#             if hasattr(language_model, 'config') and hasattr(language_model.config, 'hidden_size'):\n",
    "#                 hidden_size = language_model.config.hidden_size\n",
    "#             else:\n",
    "#                 hidden_size = 2304  # fallback\n",
    "#             activations = torch.randn(1, 64, hidden_size).to(DEVICE)\n",
    "#         except:\n",
    "#             activations = torch.randn(1, 64, 2304).to(DEVICE)\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba89ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T15:31:47.830214Z",
     "start_time": "2025-09-05T15:31:46.427416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize experiment\n",
    "experiment = LinearProbingExperiment(concept=\"cat_dog_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58ba249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stopper\n",
      "\u001b[31mNameError\u001b[39m: name 'stopper' is not defined"
     ]
    }
   ],
   "source": [
    "stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d40c7",
   "metadata": {},
   "source": [
    "# cosine_sim between probe acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1decf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T01:11:47.887578Z",
     "start_time": "2025-08-23T01:10:13.538345Z"
    }
   },
   "outputs": [],
   "source": [
    "load_models_with_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7903d34",
   "metadata": {},
   "source": [
    "# All probe exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc370a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T01:11:47.899182Z",
     "start_time": "2025-08-23T01:11:47.889683Z"
    }
   },
   "outputs": [],
   "source": [
    "import json, numpy as np, torch\n",
    "from contextlib import suppress\n",
    "\n",
    "DROP_KEYS = {\n",
    "    \"model\", \"estimator\", \"clf\", \"classifier\", \"pipeline\",\n",
    "    \"sklearn_probe\", \"probe\", \"vectorizer\", \"scaler\", \"pca\",\n",
    "    \"figure\", \"fig\", \"axes\", \"ax\", \"roc_curve_fig\", \"pr_curve_fig\"\n",
    "}\n",
    "\n",
    "def json_sanitize(obj, _visited=None):\n",
    "    if _visited is None: _visited = set()\n",
    "    oid = id(obj)\n",
    "    if oid in _visited:\n",
    "        return \"<circular_ref>\"\n",
    "    _visited.add(oid)\n",
    "\n",
    "    # Primitives\n",
    "    if obj is None or isinstance(obj, (bool, int, float, str)):\n",
    "        return obj\n",
    "\n",
    "    # NumPy / torch\n",
    "    if isinstance(obj, (np.integer, np.floating)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.detach().cpu().tolist()\n",
    "\n",
    "    # Mappings\n",
    "    if isinstance(obj, dict):\n",
    "        out = {}\n",
    "        for k, v in obj.items():\n",
    "            k_str = str(k)\n",
    "            if k_str in DROP_KEYS:\n",
    "                continue\n",
    "            out[k_str] = json_sanitize(v, _visited)\n",
    "        return out\n",
    "\n",
    "    # Sequences / Sets / Tuples\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [json_sanitize(v, _visited) for v in obj]\n",
    "\n",
    "    # sklearn classification_report (string ok), others: use repr as last resort\n",
    "    with suppress(Exception):\n",
    "        return repr(obj)\n",
    "    return str(obj)\n",
    "\n",
    "def safe_save_results(results, path, meta=None):\n",
    "    minimal = {\n",
    "        \"meta\": meta or {},\n",
    "        \"sklearn\": {\n",
    "            \"train_acc\": float(results[\"sklearn\"].get(\"train_acc\", float(\"nan\"))),\n",
    "            \"test_acc\":  float(results[\"sklearn\"].get(\"test_acc\",  float(\"nan\"))),\n",
    "            # keep dict report if available; if it’s a string it’s also fine\n",
    "            \"classification_report\": results[\"sklearn\"].get(\"classification_report\", None),\n",
    "            \"y_test\": [int(x) for x in results[\"sklearn\"].get(\"y_test\", [])],\n",
    "            \"y_pred\": [int(x) for x in results[\"sklearn\"].get(\"y_pred\", [])],\n",
    "        }\n",
    "    }\n",
    "    # merge any extra *safe* fields\n",
    "    sanitized = json_sanitize(results)\n",
    "    minimal[\"extra\"] = sanitized.get(\"extra\", {})  # optional\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(minimal, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04474e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T12:46:58.988535Z",
     "start_time": "2025-09-04T12:46:58.954356Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3960961551.py, line 43)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpadding=\"max_length\" truncation=True, max_length=max_length\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import os, gc, io, json\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# Precompute pooled activations\n",
    "# ----------------------------\n",
    "\n",
    "def _amp_ctx(device, use_amp=True):\n",
    "    \"\"\"Return a context manager for autocast if on CUDA, else a no-op.\"\"\"\n",
    "    if not use_amp:\n",
    "        return nullcontext()\n",
    "    is_cuda = torch.cuda.is_available() and (\n",
    "        str(device).startswith(\"cuda\") or getattr(getattr(device, \"type\", None), \"__str__\", lambda: \"\")() == \"cuda\"\n",
    "        or (hasattr(device, \"type\") and device.type == \"cuda\")\n",
    "    )\n",
    "    if not is_cuda:\n",
    "        return nullcontext()\n",
    "    # Prefer new API if available\n",
    "    try:\n",
    "        return torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "    except Exception:\n",
    "        # Fallback for older PyTorch\n",
    "        return torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "\n",
    "    \n",
    "def precompute_pooled_activations(\n",
    "    model, tokenizer, texts, layers, device,\n",
    "    batch_size=8, pool=\"mean\", max_length=256, use_amp=True\n",
    "):\n",
    "    model.eval()\n",
    "    acts = {l: [] for l in layers}\n",
    "\n",
    "    with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[start:start+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_texts, return_tensors=\"pt\",\n",
    "                padding=\"max_length\", truncation=True, max_length=max_length\n",
    "            )\n",
    "            \n",
    "            # To unify to no bos_token\n",
    "            if tokenizer.bos_token_id is not None:\n",
    "                enc[\"attention_mask\"][:, 0] *= (enc[\"input_ids\"][:, 0] != tokenizer.bos_token_id).to(enc[\"attention_mask\"].dtype)\n",
    "\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            out = model(**enc, output_hidden_states=True, return_dict=True)\n",
    "            hs_tuple = out.hidden_states  # (emb, layer1, ..., layerL)\n",
    "\n",
    "            mask = enc[\"attention_mask\"].unsqueeze(-1)  # (B,T,1)\n",
    "            for l in layers:\n",
    "                hs = hs_tuple[l]  # (B,T,D)\n",
    "                if pool == \"cls\":\n",
    "                    pooled = hs[:, 0, :]\n",
    "                elif pool == \"last\":\n",
    "                    last_idx = (enc[\"attention_mask\"].sum(dim=1)-1).clamp(min=0)\n",
    "                    pooled = hs[torch.arange(hs.size(0), device=hs.device), last_idx, :]\n",
    "                else:  # mean\n",
    "                    summed = (hs * mask).sum(dim=1)\n",
    "                    denom  = mask.sum(dim=1).clamp(min=1)\n",
    "                    pooled = summed / denom\n",
    "\n",
    "                acts[l].append(pooled.detach().cpu().numpy())\n",
    "\n",
    "            del out, hs_tuple, enc\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    acts = {l: np.concatenate(chunks, axis=0) for l, chunks in acts.items()}\n",
    "    return acts\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Utility: load -> precompute -> unload\n",
    "# -----------------------------------------\n",
    "def compute_acts_for_model_name(model_name, texts, layers, device):\n",
    "    model = load_models_with_eval(model_name, device)\n",
    "    if \"pali\" not in model_name:\n",
    "        tok = AutoTokenizer.from_pretrained(\"google/paligemma2-3b-pt-224\", use_fast=True)\n",
    "    else:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    acts = precompute_pooled_activations(model, tok, texts, layers, device)\n",
    "    # derive feature dim for sanity checks\n",
    "    any_layer = layers[0]\n",
    "    feat_dim = acts[any_layer].shape[1]\n",
    "    # free VRAM\n",
    "    del model, tok\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return acts, feat_dim\n",
    "\n",
    "# -----------------------------------------\n",
    "# MAIN\n",
    "# -----------------------------------------\n",
    "model1_names = [\"google/gemma-2-2b\", \"google/paligemma2-3b-pt-224\"]\n",
    "model2_names = [\"google/gemma-2-2b\", \"google/paligemma2-3b-pt-224\"]\n",
    "data_name_list = ['cat', 'dog', 'human']\n",
    "\n",
    "# Decide layers safely (take min across all models)\n",
    "# (We’ll pull num_hidden_layers from the first time each model is loaded.)\n",
    "# To avoid loading twice, we’ll compute activations right away per model.\n",
    "for texts, labels, data_name in zip(texts_list, labels_list, data_name_list):\n",
    "    print(f\"\\n=== Precomputing activations for concept: {data_name} ===\", flush=True)\n",
    "\n",
    "    # Figure out which models we need for this concept\n",
    "    needed_models = sorted(set(model1_names + model2_names))\n",
    "\n",
    "    # Decide a shared layer list AFTER we’ve inspected all model configs\n",
    "    # Strategy: compute acts per model one-by-one, track each model's L and cache acts.\n",
    "    acts_cache = {}\n",
    "    feat_dims  = {}\n",
    "    max_layers_by_model = {}\n",
    "\n",
    "    # We’ll read num_hidden_layers from config after loading; then save acts & unload model.\n",
    "    for mname in needed_models:\n",
    "        # Load once, grab num layers, compute acts, unload\n",
    "        tmp_model = load_models_with_eval(mname, DEVICE_1)\n",
    "        num_layers = int(getattr(tmp_model.config, \"num_hidden_layers\", 26))\n",
    "        del tmp_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        max_layers_by_model[mname] = num_layers\n",
    "\n",
    "    # shared 1..L across all models\n",
    "    layer_to_test = list(range(1, min(max_layers_by_model.values()) + 1))\n",
    "\n",
    "    # Now compute & cache acts per model (sequentially to save memory)\n",
    "    for mname in needed_models:\n",
    "        print(f\"  -> computing acts for {mname} (layers {layer_to_test[0]}..{layer_to_test[-1]})\", flush=True)\n",
    "        acts_cache[mname], feat_dims[mname] = compute_acts_for_model_name(\n",
    "            mname, texts, layer_to_test, DEVICE_1\n",
    "        )\n",
    "\n",
    "    # Cross-model experiments\n",
    "    for model1_name in model1_names:\n",
    "        for model2_name in model2_names:\n",
    "            print(f\"\\nRunning linear probing experiment: {data_name} | train on {model1_name.split('/')[-1]} → test on {model2_name.split('/')[-1]}\", flush=True)\n",
    "\n",
    "            all_results = []\n",
    "            train_accs, test_accs = [], []\n",
    "\n",
    "            # Optional: if feature dims mismatch, reduce both to the shared min-dim via PCA\n",
    "            d1, d2 = feat_dims[model1_name], feat_dims[model2_name]\n",
    "            shared_d = min(d1, d2)\n",
    "            use_pca = (d1 != d2)\n",
    "\n",
    "            if use_pca:\n",
    "                from sklearn.decomposition import PCA\n",
    "\n",
    "            for layer in layer_to_test:\n",
    "                X_train = acts_cache[model1_name][layer]  # (N, d1)\n",
    "                X_test  = acts_cache[model2_name][layer]  # (N, d2)\n",
    "\n",
    "                if use_pca:\n",
    "                    # Fit PCA on TRAIN only, transform both to shared_d\n",
    "                    pca = PCA(n_components=shared_d, svd_solver=\"auto\", random_state=0)\n",
    "                    X_train_reduced = pca.fit_transform(X_train)\n",
    "                    X_test_reduced  = pca.transform(X_test)\n",
    "                else:\n",
    "                    X_train_reduced, X_test_reduced = X_train, X_test\n",
    "\n",
    "                # === Run probe experiment ===\n",
    "                # Suppress inner prints if run_experiment is verbose\n",
    "                with redirect_stdout(io.StringIO()):\n",
    "                    results = experiment.run_experiment(\n",
    "                        X_train_reduced, labels,\n",
    "                        X_test_reduced,  labels\n",
    "                    )\n",
    "\n",
    "                all_results.append(results)\n",
    "\n",
    "                # Console summary\n",
    "                tr = results[1]['sklearn']['train_acc']\n",
    "                te = results[1]['sklearn']['test_acc']\n",
    "                train_accs.append(tr); test_accs.append(te)\n",
    "                print(f\"[layer {layer:02d}] train_acc={tr:.4f} | test_acc={te:.4f}\")\n",
    "\n",
    "                # Save per-layer JSON\n",
    "                os.makedirs(f\"../output/{data_name}\", exist_ok=True)\n",
    "                tag = f\"{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}\"\n",
    "                save_path = f\"../output/{data_name}/linear_probing_results_{layer}_{tag}.json\"\n",
    "                meta = {\"data\": data_name, \"layer\": layer,\n",
    "                        \"train_model\": model1_name, \"test_model\": model2_name}\n",
    "                try:\n",
    "                    safe_save_results(results, save_path, meta=meta)\n",
    "                except Exception as e:\n",
    "                    print(f\"(warn) safe_save_results failed: {e}. Writing ultra-minimal JSON.\")\n",
    "                    ultra = {\n",
    "                        \"meta\": meta,\n",
    "                        \"train_acc\": float(results[1][\"sklearn\"][\"train_acc\"]),\n",
    "                        \"test_acc\":  float(results[1][\"sklearn\"][\"test_acc\"])\n",
    "                    }\n",
    "                    with open(save_path.replace(\".json\", \"_minimal.json\"), \"w\") as f:\n",
    "                        json.dump(ultra, f, indent=2)\n",
    "\n",
    "\n",
    "                # free per-iteration temps\n",
    "                del X_train_reduced, X_test_reduced, results\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # ===== Plot Train and Test Accuracy Across Layers =====\n",
    "            layers = layer_to_test\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            plt.plot(layers, train_accs, label='Train')\n",
    "            plt.plot(layers, test_accs,  label='Test', linestyle=\"--\")\n",
    "            plt.xlabel('Layer'); plt.ylabel('Accuracy')\n",
    "            plt.title(f\"{data_name} — Train/Test accuracy by layer\\ntrain={model1_name.split('/')[-1]} → test={model2_name.split('/')[-1]}\")\n",
    "            plt.ylim(0.0, 1.0); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
    "            os.makedirs(f\"../figs_tabs/{data_name}/\", exist_ok=True)\n",
    "            plt.savefig(f\"../figs_tabs/{data_name}/acc_by_layer_{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}.png\", dpi=200)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    # Clear cached activations for this concept to free RAM\n",
    "    del acts_cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b8cc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T01:12:25.161366Z",
     "start_time": "2025-08-23T01:12:25.161358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.56it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.56it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.46 GiB. GPU 1 has a total capacity of 44.38 GiB of which 21.93 GiB is free. Process 2893533 has 22.45 GiB memory in use. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layer_to_test:\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Produce activation-label pairs\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         model1_activations = get_acts(model1, texts, layer, model1_name, DEVICE_1)\n\u001b[32m     26\u001b[39m         model2_activations = get_acts(model2, texts, layer, model2_name, DEVICE_2)\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# === Run probe experiment ===\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mget_acts\u001b[39m\u001b[34m(language_model, text, layer, model_name, DEVICE)\u001b[39m\n\u001b[32m     69\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❌ Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m out of range. Model has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(language_model.model.layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m layers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m torch.randn(\u001b[32m1\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m2304\u001b[39m).to(DEVICE), \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     activations = language_model(**inputs, output_hidden_states=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[33m'\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m activations\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m#     activations = None\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m#             activations = torch.randn(1, 64, 2304).to(DEVICE)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:599\u001b[39m, in \u001b[36mGemma2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    598\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states[:, slice_indices, :])\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.final_logit_softcapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    601\u001b[39m     logits = logits / \u001b[38;5;28mself\u001b[39m.config.final_logit_softcapping\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 22.46 GiB. GPU 1 has a total capacity of 44.38 GiB of which 21.93 GiB is free. Process 2893533 has 22.45 GiB memory in use. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model1_names = [\"google/gemma-2-2b\", \"google/paligemma2-3b-pt-224\"]\n",
    "model2_names = [\"google/gemma-2-2b\", \"google/paligemma2-3b-pt-224\"]\n",
    "data_name_list = ['cat', 'dog', 'human']\n",
    "for texts, labels, data_name in zip(texts_list, labels_list, data_name_list):\n",
    "    for model1_name in model1_names:\n",
    "        for model2_name in model2_names:\n",
    "\n",
    "            model1 = load_models_with_eval(model1_name, DEVICE_1)\n",
    "            model2 = load_models_with_eval(model2_name, DEVICE_2)\n",
    "\n",
    "            # # Dummy activations (replace with real data)\n",
    "            # gemma_activations = np.random.randn(1000, 768)  # 1000 samples, 768 dims\n",
    "            # gemma_labels = np.random.randint(0, 2, 1000)    # binary cat/dog labels\n",
    "\n",
    "            # polygemma_activations = np.random.randn(200, 768)  # 200 test samples\n",
    "            # polygemma_labels = np.random.randint(0, 2, 200)\n",
    "\n",
    "            layer_to_test = list(range(1, 27))\n",
    "            all_results = []\n",
    "            # Run experiment per layer\n",
    "            for layer in layer_to_test:\n",
    "                # Produce activation-label pairs\n",
    "                with torch.inference_mode():\n",
    "                    model1_activations = get_acts(model1, texts, layer, model1_name, DEVICE_1)\n",
    "                    model2_activations = get_acts(model2, texts, layer, model2_name, DEVICE_2)\n",
    "\n",
    "                # === Run probe experiment ===\n",
    "                results = experiment.run_experiment(\n",
    "                    model1_activations, labels,\n",
    "                    model2_activations, labels\n",
    "                )\n",
    "    \n",
    "                all_results.append(results)\n",
    "\n",
    "                print(f\"\\n=== LAYER {layer} RESULTS ===\")\n",
    "                print(f\"Train Accuracy: {results['sklearn']['train_acc']:.4f}\")\n",
    "                print(f\"Test Accuracy: {results['sklearn']['test_acc']:.4f}\")\n",
    "                print(\"Classification Report:\")\n",
    "                print(results['sklearn']['classification_report'])\n",
    "                      \n",
    "                 \n",
    "                      \n",
    "                # Save results\n",
    "                os.makedirs(f\"../output/{data_name}\", exist_ok=True)\n",
    "                experiment.save_results(results, f\"../output/{data_name}/linear_probing_results_{layer}_{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}.json\")\n",
    "\n",
    "                # Save misclassified examples\n",
    "                if 'y_test' in results['sklearn'] and 'y_pred' in results['sklearn']:\n",
    "                    mis_indices = [i for i, (yt, yp) in enumerate(zip(\n",
    "                        results['sklearn']['y_test'], results['sklearn']['y_pred']\n",
    "                    )) if yt != yp]\n",
    "                    mis_info = [\n",
    "                        {\n",
    "                            'text': cat_texts[i],  # assuming you meant cat_texts not text\n",
    "                            'true_label': int(results['sklearn']['y_test'][i]),\n",
    "                            'pred_label': int(results['sklearn']['y_pred'][i])\n",
    "                        }\n",
    "                        for i in mis_indices\n",
    "                    ]\n",
    "                    os.makedirs(f\"../output/{data_name}/{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}\", exist_ok=True)\n",
    "                    with open(f\"../output/{data_name}/{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}/misclassified_layer_{layer}.json\", 'w') as f:\n",
    "                        json.dump(mis_info, f, indent=2)\n",
    "                else:\n",
    "                    print(\"Warning: 'y_test' or 'y_pred' not found in results; misclassifications not saved.\")\n",
    "\n",
    "                del model1_activations, model2_activations, results\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # ===== Plot Train and Test Accuracy Across Layers =====\n",
    "\n",
    "            layers = layer_to_test\n",
    "            train_accs = [res['sklearn']['train_acc'] for res in all_results]\n",
    "            test_accs = [res['sklearn']['test_acc'] for res in all_results]\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(layers, train_accs)\n",
    "            plt.plot(layers, test_accs)\n",
    "            plt.xlabel('Layer')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f\"Train and Test Accuracy by Layer_{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}. Concept: {data_name}\")\n",
    "            plt.legend(['Train', 'Test'])\n",
    "            os.makedirs(f\"../figs_tabs/{data_name}/\", exist_ok=True)\n",
    "            plt.savefig(f\"../figs_tabs/{data_name}/Linear Probe Train and Test Accuracy by Layer_{model1_name.split('/')[-1]}_{model2_name.split('/')[-1]}.png\")\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a3575",
   "metadata": {},
   "source": [
    "# All exps (image to pali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NEW imports (top of file)\n",
    "# from contextlib import nullcontext\n",
    "# from PIL import Image\n",
    "# from transformers import AutoProcessor\n",
    "# import concurrent.futures\n",
    "\n",
    "# # NEW: preload compressed image bytes into RAM once per dataset\n",
    "# def _read_bytes(path: str) -> bytes:\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def preload_image_bytes(filenames, root, max_workers: int = 12):\n",
    "#     paths = [os.path.join(root, fn) for fn in filenames]\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "#         data = list(ex.map(_read_bytes, paths))\n",
    "#     return dict(zip(filenames, data))\n",
    "# # NEW imports (top of file)\n",
    "# from contextlib import nullcontext\n",
    "# from PIL import Image\n",
    "# from transformers import AutoProcessor\n",
    "# import concurrent.futures\n",
    "\n",
    "# # NEW: preload compressed image bytes into RAM once per dataset\n",
    "# def _read_bytes(path: str) -> bytes:\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def preload_image_bytes(filenames, root, max_workers: int = 12):\n",
    "#     paths = [os.path.join(root, fn) for fn in filenames]\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "#         data = list(ex.map(_read_bytes, paths))\n",
    "#     return dict(zip(filenames, data))\n",
    "\n",
    "# def compute_acts_for_model_name(\n",
    "#     model_name, texts, layers, device,\n",
    "#     img_filenames=None, img_byte_cache=None, pool=\"mean\"\n",
    "# ):\n",
    "#     model = load_models_with_eval(model_name, device)\n",
    "\n",
    "#     if \"pali\" in model_name.lower():\n",
    "#         preproc = AutoProcessor.from_pretrained(model_name)\n",
    "#         acts = precompute_pooled_activations(\n",
    "#             model, preproc, texts, layers, device,\n",
    "#             pool=pool, is_pali=True,\n",
    "#             img_filenames=img_filenames, img_byte_cache=img_byte_cache\n",
    "#         )\n",
    "#         any_layer = layers[0]; feat_dim = acts[any_layer].shape[1]\n",
    "#         del model, preproc\n",
    "#     else:\n",
    "#         tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "#         acts = precompute_pooled_activations(\n",
    "#             model, tok, texts, layers, device, pool=pool\n",
    "#         )\n",
    "#         any_layer = layers[0]; feat_dim = acts[any_layer].shape[1]\n",
    "#         del model, tok\n",
    "\n",
    "#     gc.collect(); torch.cuda.empty_cache()\n",
    "#     return acts, feat_dim\n",
    "\n",
    "\n",
    "\n",
    "# # Figure out max layers per model (decoder for Gemma, VISION for PaliGemma)\n",
    "# for mname in needed_models:\n",
    "#     tmp_model = load_models_with_eval(mname, DEVICE_1)\n",
    "#     if \"pali\" in mname.lower():\n",
    "#         num_layers = int(getattr(tmp_model.vision_tower.config, \"num_hidden_layers\",\n",
    "#                           getattr(tmp_model.vision_tower.config, \"num_layers\", 24)))\n",
    "#     else:\n",
    "#         num_layers = int(getattr(tmp_model.config, \"num_hidden_layers\", 26))\n",
    "#     del tmp_model\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     max_layers_by_model[mname] = num_layers\n",
    "\n",
    "# layer_to_test = list(range(1, min(max_layers_by_model.values()) + 1))\n",
    "\n",
    "# IMG_ROOT =  f\"../data/coco_val2017_{data_name}_binary_with_captions_balanced_images\" \n",
    "\n",
    "# for texts, labels, data_name, filenames in zip(texts_list, labels_list, data_name_list, filenames_list):\n",
    "#     print(f\"\\n=== Precomputing activations for concept: {data_name} ===\", flush=True)\n",
    "\n",
    "#     # Build one byte cache per dataset (reused across models)\n",
    "#     img_byte_cache = preload_image_bytes(filenames, IMG_ROOT, max_workers=12)\n",
    "\n",
    "#     needed_models = sorted(set(model1_names + model2_names))\n",
    "#     acts_cache = {}\n",
    "#     feat_dims  = {}\n",
    "#     max_layers_by_model = {}\n",
    "\n",
    "#     # (layer discovery block from Section 4 here)\n",
    "\n",
    "#     # Now compute & cache acts per model\n",
    "#     for mname in needed_models:\n",
    "#         print(f\"  -> computing acts for {mname} (layers {layer_to_test[0]}..{layer_to_test[-1]})\", flush=True)\n",
    "#         if \"pali\" in mname.lower():\n",
    "#             acts_cache[mname], feat_dims[mname] = compute_acts_for_model_name(\n",
    "#                 mname, texts=None, layers=layer_to_test, device=DEVICE_1,\n",
    "#                 img_filenames=filenames, img_byte_cache=img_byte_cache, pool=\"mean\"\n",
    "#             )\n",
    "#         else:\n",
    "#             acts_cache[mname], feat_dims[mname] = compute_acts_for_model_name(\n",
    "#                 mname, texts=texts, layers=layer_to_test, device=DEVICE_1, pool=\"mean\"\n",
    "#             )\n",
    "\n",
    "#     # ... the rest of your cross-model experiment loop stays unchanged ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a139328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:06:47.987335Z",
     "start_time": "2025-09-05T15:47:47.760317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concept: cat ===\n",
      "\n",
      "Running linear probe: cat | train on gemma-2-2b → test on gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 58.44it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 368, 256, 2304])\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 01 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 02 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 03 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 04 | train_acc=1.0000 | test_acc=0.9459\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 05 | train_acc=1.0000 | test_acc=0.9459\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 06 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 07 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 08 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 09 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 10 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 11 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 12 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cat] layer 13 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 14 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 15 | train_acc=1.0000 | test_acc=0.9459\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 16 | train_acc=1.0000 | test_acc=0.9459\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 17 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 18 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 19 | train_acc=1.0000 | test_acc=0.9459\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 20 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 21 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 22 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 23 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 24 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n",
      "[cat] layer 25 | train_acc=1.0000 | test_acc=0.9595\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([368, 256, 2304])\n",
      "PolyGemma test data: torch.Size([368, 256, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([294, 256, 2304]), size of gemma_labels_train: 294\n",
      "size of polygemma_activations_test: torch.Size([74, 256, 2304]), size of polygemma_labels_test: 74\n",
      "X_train_flat shape: torch.Size([294, 589824])\n",
      "y_train shape: 294\n",
      "X_test_flat shape: torch.Size([74, 589824])\n",
      "y_test shape: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cat] layer 26 | train_acc=1.0000 | test_acc=0.9459\n",
      "(warn) save_results failed (Circular reference detected); writing minimal JSON.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running linear probe: cat | train on gemma-2-2b → test on paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.80it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tok' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 218\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m    212\u001b[39m     act1 = get_acts(\n\u001b[32m    213\u001b[39m         model = model1, texts = texts, model_name = model1_name, device = DEVICE_1,\n\u001b[32m    214\u001b[39m         filenames=filenames \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpali\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model1_name.lower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    215\u001b[39m         img_byte_cache=img_byte_cache \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpali\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model1_name.lower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    216\u001b[39m         batch_size=BATCH_SIZE, use_amp=USE_AMP\n\u001b[32m    217\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     act2 = get_acts(\n\u001b[32m    219\u001b[39m         model = model2, texts = texts, model_name = model2_name, device = DEVICE_2,\n\u001b[32m    220\u001b[39m         filenames=filenames \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpali\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model2_name.lower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    221\u001b[39m         img_byte_cache=img_byte_cache \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpali\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model2_name.lower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    222\u001b[39m         batch_size=BATCH_SIZE, use_amp=USE_AMP\n\u001b[32m    223\u001b[39m     )\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mprint\u001b[39m(act1.shape)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layer_to_test:\n\u001b[32m    226\u001b[39m \n\u001b[32m    227\u001b[39m \n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# === Run probe experiment ===\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mget_acts\u001b[39m\u001b[34m(model, texts, model_name, device, filenames, img_byte_cache, batch_size, use_amp, max_length)\u001b[39m\n\u001b[32m    129\u001b[39m                 imgs = [Image.open(io.BytesIO(img_byte_cache[fn])).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m fnb]\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m#                 imgs = [Image.open(fp).convert(\"RGB\") for fp in fbatch]\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m#                 enc = proc(images=imgs, return_tensors=\"pt\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m                 enc = proc(images=imgs, text=[\u001b[33m\"\u001b[39m\u001b[33m<image>\u001b[39m\u001b[33m\"\u001b[39m]*\u001b[38;5;28mlen\u001b[39m(imgs), return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m    134\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m tok.bos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    135\u001b[39m                     enc[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m][:, \u001b[32m0\u001b[39m] *= (enc[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][:, \u001b[32m0\u001b[39m] != tok.bos_token_id).to(enc[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].dtype)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'tok' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# ========= helpers for this loop =========\n",
    "from contextlib import nullcontext\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "import concurrent.futures\n",
    "import io\n",
    "def _amp_ctx(device, use_amp=True):\n",
    "    \"\"\"Return a context manager for autocast if on CUDA, else a no-op.\"\"\"\n",
    "    if not use_amp:\n",
    "        return nullcontext()\n",
    "    is_cuda = torch.cuda.is_available() and (\n",
    "        str(device).startswith(\"cuda\") or getattr(getattr(device, \"type\", None), \"__str__\", lambda: \"\")() == \"cuda\"\n",
    "        or (hasattr(device, \"type\") and device.type == \"cuda\")\n",
    "    )\n",
    "    if not is_cuda:\n",
    "        return nullcontext()\n",
    "    # Prefer new API if available\n",
    "    try:\n",
    "        return torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "    except Exception:\n",
    "        # Fallback for older PyTorch\n",
    "        return torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "    \n",
    "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".webp\")\n",
    "\n",
    "def list_filenames(img_root):\n",
    "    return sorted([fn for fn in os.listdir(img_root) if fn.lower().endswith(IMG_EXTS)])\n",
    "# --- add this helper ---\n",
    "\n",
    "def _read_bytes(path: str) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def preload_image_bytes(filenames, root, max_workers: int = 12):\n",
    "    paths = [os.path.join(root, fn) for fn in filenames]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        data = list(ex.map(_read_bytes, paths))\n",
    "    return dict(zip(filenames, data))\n",
    "\n",
    "# REPLACE your get_num_layers(...) with this\n",
    "def get_num_layers(model):\n",
    "    \"\"\"\n",
    "    Return count of indexable hidden-state layers:\n",
    "      - If the model has a langauge model, return its depth.\n",
    "      - Otherwise, return decoder/text depth from model.config.\n",
    "    Assumes hidden_states = (emb, layer1, ..., layerL) → valid indices 1..L.\n",
    "    \"\"\"\n",
    "    lm = getattr(model, \"language_model\", None)\n",
    "    if lm is not None:\n",
    "        # Some VLMs keep the tower in a list/tuple\n",
    "        if isinstance(lm, (list, tuple)):\n",
    "            lm = lm[0]\n",
    "        cfg = getattr(lm, \"config\", None)\n",
    "        return int(getattr(cfg, \"num_hidden_layers\", getattr(cfg, \"num_layers\", 24)))\n",
    "    # Text-only models\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "    return int(getattr(cfg, \"num_hidden_layers\", 26))\n",
    "\n",
    "\n",
    "_tokenizer_cache = {}\n",
    "_processor_cache = {}\n",
    "\n",
    "def _get_tokenizer(model_name):\n",
    "    if model_name not in _tokenizer_cache:\n",
    "        _tokenizer_cache[model_name] = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    return _tokenizer_cache[model_name]\n",
    "\n",
    "def _get_processor(model_name):\n",
    "    if model_name not in _processor_cache:\n",
    "        _processor_cache[model_name] = AutoProcessor.from_pretrained(model_name)\n",
    "    return _processor_cache[model_name]\n",
    "\n",
    "def _short(name: str):\n",
    "    return name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "\n",
    "def get_acts(\n",
    "    model, texts, model_name, device,\n",
    "    *,\n",
    "    filenames=None, img_byte_cache=None,  # only for PaliGemma\n",
    "    batch_size=32, use_amp=True, max_length=256\n",
    "):\n",
    "    \"\"\"Return (N, D) activations at a specific layer, pooled.\n",
    "       - Gemma: text forward, layer over decoder hs\n",
    "       - PaliGemma: vision_tower forward, layer over vision hs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    feats = []\n",
    "\n",
    "    if not hasattr(model, \"vision_tower\"):\n",
    "        # ------- TEXT path (Gemma) ------- 2304 d_hidden\n",
    "        tok = _get_tokenizer(model_name)\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for start in range(0, len(texts), batch_size):\n",
    "                batch = texts[start:start+batch_size]\n",
    "                enc = tok(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "                \n",
    "                if tok.bos_token_id is not None:\n",
    "                    enc[\"attention_mask\"][:, 0] *= (enc[\"input_ids\"][:, 0] != tok.bos_token_id).to(enc[\"attention_mask\"].dtype)\n",
    "                    \n",
    "                enc = {k: v.to(device) for k, v in enc.items()}\n",
    "                out = model(**enc, output_hidden_states=True, return_dict=True)\n",
    "                hs_tuple = out.hidden_states  # L * (B, T, D) \n",
    "#                 mask = enc[\"attention_mask\"].unsqueeze(-1)  # (B, T, 1)\n",
    "#                 if pool == \"cls\":\n",
    "#                     pooled = hs[:, 0, :]\n",
    "#                 elif pool == \"last\":\n",
    "#                     last_idx = (enc[\"attention_mask\"].sum(dim=1) - 1).clamp(min=0)\n",
    "#                     pooled = hs[torch.arange(hs.size(0), device=hs.device), last_idx, :]\n",
    "#                 else:  # mean\n",
    "#                     summed = (hs * mask).sum(dim=1)\n",
    "#                     denom = mask.sum(dim=1).clamp(min=1)\n",
    "#                     pooled = summed / denom\n",
    "                feats.append([h.detach().cpu().float() for h in hs_tuple])\n",
    "                del out, enc, hs_tuple\n",
    "                torch.cuda.empty_cache()\n",
    "    else:\n",
    "        # ------- IMAGE path (PaliGemma) ------- 2304 d_hidden\n",
    "        assert filenames is not None and img_byte_cache is not None, \"PaliGemma path needs filenames & img_byte_cache.\"\n",
    "        proc = _get_processor(model_name)\n",
    "        tok = _get_tokenizer(model_name)\n",
    "        # make sure the tower emits hidden states\n",
    "        try:\n",
    "            model.vision_tower.config.output_hidden_states = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for start in range(0, len(filenames), batch_size):\n",
    "                fnb = filenames[start:start+batch_size]\n",
    "                imgs = [Image.open(io.BytesIO(img_byte_cache[fn])).convert(\"RGB\") for fn in fnb]\n",
    "#                 imgs = [Image.open(fp).convert(\"RGB\") for fp in fbatch]\n",
    "#                 enc = proc(images=imgs, return_tensors=\"pt\")\n",
    "                \n",
    "                enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\").to(device)\n",
    "                if tok.bos_token_id is not None:\n",
    "                    enc[\"attention_mask\"][:, 0] *= (enc[\"input_ids\"][:, 0] != tok.bos_token_id).to(enc[\"attention_mask\"].dtype)\n",
    "\n",
    "                out = model(\n",
    "                            **enc,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True\n",
    "                        )\n",
    "\n",
    "                hs_tuple = out.hidden_states   # tuple(len = n_layers), each [B, seq, 2304]\n",
    "                print(f\"hs_tuple[0].shape: {hs_tuple[0].shape}\")\n",
    "                \n",
    "#                 if pool == \"cls\" and hs.size(1) >= 1:\n",
    "#                     pooled = hs[:, 0, :]\n",
    "#                 elif pool == \"last\" and hs.size(1) >= 1:\n",
    "#                     pooled = hs[:, -1, :]\n",
    "#                 else:\n",
    "#                     pooled = hs.mean(dim=1)  # robust across ViT/SigLIP-like towers\n",
    "#                 feats.append(pooled.detach().float().cpu().numpy())\n",
    "                feats.append([h.detach().cpu().float() for h in hs_tuple])\n",
    "                del out, enc, imgs, hs_tuple\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    n_layers = len(feats[0])\n",
    "    layerwise = []\n",
    "    for l in range(n_layers):\n",
    "        tensors = [batch[l] for batch in feats]    # list of [B, seq, d_hid]\n",
    "        layerwise.append(torch.cat(tensors, dim=0))  # L * [N, seq, d_hid] (consistent seq if padded)\n",
    "\n",
    "    return torch.stack(layerwise, dim=0) # stack: L * [N, seq, d_hid] -> [L, N, seq, d_hid]\n",
    "# ========= end helpers =========\n",
    "\n",
    "\n",
    "# =======================\n",
    "#        MAIN LOOP\n",
    "# =======================\n",
    "model1_names = [\"google/gemma-2-2b\", \"google/paligemma2-3b-pt-224\"]\n",
    "model2_names = [\"google/gemma-2-2b\", \"google/paligemma2-3b-pt-224\"]\n",
    "data_name_list = ['cat', 'dog', 'human']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "POOL = \"mean\"      # or \"cls\"/\"last\"\n",
    "USE_AMP = True\n",
    "experiment = LinearProbingExperiment(concept=\"cat_dog_classification\")\n",
    "\n",
    "for texts, labels, data_name in zip(texts_list, labels_list, data_name_list):\n",
    "    print(f\"\\n=== Concept: {data_name} ===\", flush=True)\n",
    "\n",
    "    # --- Per-concept image root + filenames + RAM cache (used only by PaliGemma) ---\n",
    "    IMG_ROOT = f\"../data/coco_val2017_{data_name}_binary_with_captions_balanced_images\"\n",
    "    filenames = list_filenames(IMG_ROOT)\n",
    "\n",
    "    # Align counts conservatively across texts/labels/images\n",
    "    N = min(len(texts), len(labels), len(filenames)) if len(filenames) > 0 else min(len(texts), len(labels))\n",
    "    if N < len(texts) or N < len(labels):\n",
    "        print(f\"(warn) Truncating to N={N} to keep text/label/image lengths aligned.\", flush=True)\n",
    "    texts   = texts[:N]\n",
    "    labels  = labels[:N]\n",
    "    filenames = filenames[:N]\n",
    "\n",
    "    img_byte_cache = preload_image_bytes(filenames, IMG_ROOT, max_workers=12) if len(filenames) else None\n",
    "\n",
    "    for model1_name in model1_names:\n",
    "        for model2_name in model2_names:\n",
    "            print(f\"\\nRunning linear probe: {data_name} | train on {_short(model1_name)} → test on {_short(model2_name)}\", flush=True)\n",
    "\n",
    "            model1 = load_models_with_eval(model1_name, DEVICE_1, return_with_vision_tower=True)\n",
    "            model2 = load_models_with_eval(model2_name, DEVICE_2, return_with_vision_tower=True)\n",
    "\n",
    "            # --- Shared layer range (decoder for Gemma, decoder for PaliGemma) ---\n",
    "            L1 = get_num_layers(model1)\n",
    "            L2 = get_num_layers(model2)\n",
    "            layer_to_test = list(range(1, min(L1, L2) + 1))\n",
    "\n",
    "            all_results = []\n",
    "            train_accs, test_accs = [], []\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                act1 = get_acts(\n",
    "                    model = model1, texts = texts, model_name = model1_name, device = DEVICE_1,\n",
    "                    filenames=filenames if \"pali\" in model1_name.lower() else None,\n",
    "                    img_byte_cache=img_byte_cache if \"pali\" in model1_name.lower() else None,\n",
    "                    batch_size=BATCH_SIZE, use_amp=USE_AMP\n",
    "                )\n",
    "                act2 = get_acts(\n",
    "                    model = model2, texts = texts, model_name = model2_name, device = DEVICE_2,\n",
    "                    filenames=filenames if \"pali\" in model2_name.lower() else None,\n",
    "                    img_byte_cache=img_byte_cache if \"pali\" in model2_name.lower() else None,\n",
    "                    batch_size=BATCH_SIZE, use_amp=USE_AMP\n",
    "                )\n",
    "                print(act1.shape)\n",
    "            for layer in layer_to_test:\n",
    "                \n",
    "\n",
    "                # === Run probe experiment ===\n",
    "                results = experiment.run_experiment(\n",
    "                    act1[layer], labels,\n",
    "                    act2[layer], labels\n",
    "                )\n",
    "                all_results.append(results)\n",
    "\n",
    "                tr = results[1]['sklearn']['train_acc']\n",
    "                te = results[1]['sklearn']['test_acc']\n",
    "                train_accs.append(tr); test_accs.append(te)\n",
    "\n",
    "                print(f\"[{data_name}] layer {layer:02d} | train_acc={tr:.4f} | test_acc={te:.4f}\")\n",
    "                # Print (optional) report line\n",
    "                # print(results[1]['sklearn']['classification_report'])\n",
    "\n",
    "                # ---- Save results ----\n",
    "                os.makedirs(f\"../output/{data_name}\", exist_ok=True)\n",
    "                tag = f\"{_short(model1_name)}_{_short(model2_name)}_img4pali\"\n",
    "                out_json = f\"../output/{data_name}/linear_probing_results_layer{layer:02d}_{tag}.json\"\n",
    "                try:\n",
    "                    experiment.save_results(results, out_json)\n",
    "                except Exception as e:\n",
    "                    # Fallback minimal JSON if your experiment wrapper expects different fields\n",
    "                    print(f\"(warn) save_results failed ({e}); writing minimal JSON.\")\n",
    "                    ultra = {\n",
    "                        \"meta\": {\"data\": data_name, \"layer\": int(layer),\n",
    "                                 \"train_model\": model1_name, \"test_model\": model2_name},\n",
    "                        \"train_acc\": float(tr),\n",
    "                        \"test_acc\":  float(te)\n",
    "                    }\n",
    "                    with open(out_json.replace(\".json\", \"_minimal.json\"), \"w\") as f:\n",
    "                        json.dump(ultra, f, indent=2)\n",
    "\n",
    "                # ---- Save misclassified examples (use current 'texts') ----\n",
    "                if 'y_test' in results[1]['sklearn'] and 'y_pred' in results[1]['sklearn']:\n",
    "                    mis_indices = [i for i, (yt, yp) in enumerate(zip(\n",
    "                        results[1]['sklearn']['y_test'], results[1]['sklearn']['y_pred']\n",
    "                    )) if yt != yp]\n",
    "                    mis_info = [\n",
    "                        {\n",
    "                            'text': texts[i],\n",
    "                            'true_label': int(results[1]['sklearn']['y_test'][i]),\n",
    "                            'pred_label': int(results[1]['sklearn']['y_pred'][i])\n",
    "                        }\n",
    "                        for i in mis_indices\n",
    "                    ]\n",
    "                    mis_dir = f\"../output/{data_name}/{tag}\"\n",
    "                    os.makedirs(mis_dir, exist_ok=True)\n",
    "                    with open(os.path.join(mis_dir, f\"misclassified_layer_{layer:02d}_img4pali.json\"), \"w\") as f:\n",
    "                        json.dump(mis_info, f, indent=2)\n",
    "                else:\n",
    "                    print(\"(warn) 'y_test' or 'y_pred' not found; misclassifications not saved.\")\n",
    "\n",
    "                del results\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            # ===== Plot Train and Test Accuracy Across Layers =====\n",
    "            layers_ = layer_to_test\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            plt.plot(layers_, train_accs, label='Train')\n",
    "            plt.plot(layers_, test_accs,  label='Test', linestyle=\"--\")\n",
    "            plt.xlabel('Layer'); plt.ylabel('Accuracy')\n",
    "            plt.title(f\"{data_name} — Linear Probe Accuracy by Layer\\ntrain={_short(model1_name)} → test={_short(model2_name)}\\n_img4pali\")\n",
    "            plt.ylim(0.0, 1.0); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()\n",
    "            fig_dir = f\"../figs_tabs/{data_name}/\"\n",
    "            os.makedirs(fig_dir, exist_ok=True)\n",
    "            plt.savefig(os.path.join(fig_dir, f\"acc_by_layer_{_short(model1_name)}_{_short(model2_name)}_img4pali.png\"), dpi=200)\n",
    "            plt.show(); plt.close()\n",
    "\n",
    "            # free models\n",
    "            del model1, model2\n",
    "            gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a6dd8ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T19:27:43.092420Z",
     "start_time": "2025-09-04T19:27:42.959717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((368, 256, 2304),\n",
       " array([[ 605.30035  , -564.04877  , 1296.809    , ...,  352.9921   ,\n",
       "          871.9574   , -577.63043  ],\n",
       "        [ 605.30035  , -564.04877  , 1296.809    , ...,  352.9921   ,\n",
       "          871.9574   , -577.63043  ],\n",
       "        [ 605.30035  , -564.04877  , 1296.809    , ...,  352.9921   ,\n",
       "          871.9574   , -577.63043  ],\n",
       "        ...,\n",
       "        [-209.13249  ,   13.0234995, -276.55276  , ...,  -55.724533 ,\n",
       "           37.18962  ,  282.20236  ],\n",
       "        [ -85.51965  ,   69.98944  , -254.6207   , ...,   15.276027 ,\n",
       "           44.404224 ,   74.335    ],\n",
       "        [-137.39641  ,  237.75563  , -464.29324  , ..., -420.00986  ,\n",
       "          -13.945138 ,   46.029613 ]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1[0].shape, act1[0].sum(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

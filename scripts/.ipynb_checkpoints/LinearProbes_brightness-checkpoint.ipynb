{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca507af",
   "metadata": {},
   "source": [
    "# LinearProbes_brightness\n",
    "\n",
    "Pairwise-controlled linear probing for **brightness** (−50% vs +50%) using COCO val2017 and PaliGemma.\n",
    "\n",
    "> Labels: 0 = darker (−50%), 1 = brighter (+50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7ed28",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.081Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Delete specific tensors if you have them\n",
    "# del your_tensor_variable\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Check VRAM usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"VRAM reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e0868",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.083Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## 0. Config & Setup\n",
    "# Adjust paths as needed. This notebook uses COCO **val2017** only.\n",
    "\n",
    "import os, random, json, io\n",
    "from pathlib import Path\n",
    "\n",
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "SEED = 42\n",
    "def seed_everywhere(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)   \n",
    "seed_everywhere(SEED)\n",
    "COEFFICIENTS = '_[0.5, 1.5]'\n",
    "# --- Paths (edit these to match your local files) ---\n",
    "# ANNO_DIR = '../data/annotations_trainval2017/annotations'\n",
    "# IMG_DIR  = '../data/val2017'               # COCO val2017 images\n",
    "OUT_IMG_DIR = f'../data/brightness_pairs{COEFFICIENTS}'   # directory to save brightness-perturbed images\n",
    "OUT_CSV = f'../data/brightness_dataset{COEFFICIENTS}.csv' # CSV with variants & labels\n",
    "OUTPUT_DIR = f'../figs_tabs/brightness_probe_pairwise{COEFFICIENTS}'\n",
    "os.makedirs(OUT_IMG_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Probe params\n",
    "N_GROUPS = 200                # number of base images to sample (each yields 2 variants)\n",
    "PAD_TO_MAX = 64               # text max length when extracting LM activations\n",
    "MODE = \"lm\"                   # 'lm' 2304 or 'raw' 1152 (vision); we look at the representation difference in LM\n",
    "MODEL_NAME = 'google/paligemma2-3b-pt-224'\n",
    "\n",
    "print('Config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd9c0f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.084Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## 1. Environment Check\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkg):\n",
    "    try:\n",
    "        __import__(pkg.split('==')[0].split('[')[0].replace('-', '_'))\n",
    "    except Exception:\n",
    "        print(f'Installing {pkg} ...')\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# Ensure deps (comment out if you manage env separately)\n",
    "for pkg in [\n",
    "    \"pycocotools\",\n",
    "    \"transformers>=4.41.0\",\n",
    "    \"torch\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"Pillow\",\n",
    "]:\n",
    "    try:\n",
    "        __import__(pkg.split('>=')[0].split('==')[0])\n",
    "    except Exception as e:\n",
    "        pip_install(pkg)\n",
    "\n",
    "print('Environment ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd7748",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.086Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, AutoProcessor\n",
    "from typing import List, Optional\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)\n",
    "\n",
    "# AMP context helper\n",
    "class amp_ctx:\n",
    "    def __init__(self, device='cuda', use_amp=True):\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp and (device == 'cuda')\n",
    "    def __enter__(self):\n",
    "        if self.use_amp:\n",
    "            self.ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)\n",
    "            self.ctx.__enter__()\n",
    "        else:\n",
    "            self.ctx = None\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if self.ctx is not None:\n",
    "            self.ctx.__exit__(exc_type, exc, tb)\n",
    "\n",
    "# Load model\n",
    "MODEL_NAME = \"google/paligemma2-3b-pt-224\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "print('Model loaded:', MODEL_NAME)\n",
    "\n",
    "# --- MINIMAL STEERING CODE ---\n",
    "\n",
    "def get_steering_vector(model, tokenizer, pos_prompt: str, neg_prompt: str, layer_idx: int = 15):\n",
    "    \"\"\"Extract steering vector from positive/negative prompt pair\"\"\"\n",
    "    \n",
    "    # Tokenize prompts\n",
    "    pos_inputs = tokenizer(pos_prompt, return_tensors=\"pt\").to(device)\n",
    "    neg_inputs = tokenizer(neg_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Forward pass with hooks to capture activations\n",
    "    pos_acts = []\n",
    "    neg_acts = []\n",
    "    \n",
    "    def hook_fn(acts_list):\n",
    "        def hook(module, input, output):\n",
    "            acts_list.append(output[0].detach().clone())  # hidden states\n",
    "        return hook\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with amp_ctx(device):\n",
    "            # Get positive activations\n",
    "            handle = model.language_model.layers[layer_idx].register_forward_hook(hook_fn(pos_acts))\n",
    "            _ = model.language_model(**pos_inputs)\n",
    "            handle.remove()\n",
    "            \n",
    "            # Get negative activations  \n",
    "            handle = model.language_model.layers[layer_idx].register_forward_hook(hook_fn(neg_acts))\n",
    "            _ = model.language_model(**neg_inputs)\n",
    "            handle.remove()\n",
    "    \n",
    "    # Steering vector = difference in activations (at last token position)\n",
    "    pos_act = pos_acts[0][0, -1, :]  # [hidden_dim] at the last seq pos\n",
    "    neg_act = neg_acts[0][0, -1, :]  # [hidden_dim] \n",
    "    steering_vec = pos_act - neg_act\n",
    "    \n",
    "    return steering_vec\n",
    "\n",
    "def apply_steering(model, inputs, steering_vec: torch.Tensor, layer_idx: int = 15, strength: float = 1.0):\n",
    "    \"\"\"Apply steering vector during generation\"\"\"\n",
    "    device = inputs[\"input_ids\"].device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Ensure steering vec matches device & dtype\n",
    "    steering_vec = steering_vec.to(device=device, dtype=dtype)\n",
    "    def steering_hook(module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        # Add steering to last token position\n",
    "        hidden_states[0, -1, :] += strength * steering_vec # steer at the last seq pos\n",
    "        return (hidden_states,) + output[1:]\n",
    "    \n",
    "    # Register hook\n",
    "    handle = model.language_model.layers[layer_idx].register_forward_hook(steering_hook)\n",
    "    \n",
    "    try:\n",
    "        with amp_ctx(device):\n",
    "            outputs = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac6c12",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create steering vector for \"happy\" vs \"sad\" sentiment\n",
    "steering_vec = get_steering_vector(\n",
    "    model, tokenizer,\n",
    "    pos_prompt=\"A very bright picture of a cat sitting on a sofa\",\n",
    "    neg_prompt=\"A very dark picture of a cat sitting on a sofa\",\n",
    "    layer_idx=15\n",
    ")\n",
    "\n",
    "# Test steering on a neutral prompt\n",
    "test_prompt = \"Answer in [bright, dark]. Sam looks out of the window - The sky is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nOriginal generation:\")\n",
    "with torch.no_grad():\n",
    "    with amp_ctx(device):\n",
    "        orig_out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(orig_out[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nWith positive steering (strength=2.0):\")\n",
    "steered_out = apply_steering(model, inputs, steering_vec, layer_idx=15, strength=2)\n",
    "print(tokenizer.decode(steered_out[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nWith negative steering (strength=-2.0):\")\n",
    "steered_out = apply_steering(model, inputs, steering_vec, layer_idx=15, strength=-2)\n",
    "print(tokenizer.decode(steered_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5ab32",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.088Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## 4. Activation Extraction (with pad_to_max + 'lm'/'raw' modes)\n",
    "\n",
    "# Returns list of arrays per layer: [N, seq_len, D]\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def _amp_ctx(device, use_amp=True):\n",
    "    \"\"\"Return a context manager for autocast if on CUDA, else a no-op.\"\"\"\n",
    "    if not use_amp:\n",
    "        return nullcontext()\n",
    "    is_cuda = torch.cuda.is_available() and (\n",
    "        str(device).startswith(\"cuda\") or getattr(getattr(device, \"type\", None), \"__str__\", lambda: \"\")() == \"cuda\"\n",
    "        or (hasattr(device, \"type\") and device.type == \"cuda\")\n",
    "    )\n",
    "    if not is_cuda:\n",
    "        return nullcontext()\n",
    "    # Prefer new API if available\n",
    "    try:\n",
    "        return torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "    except Exception:\n",
    "        # Fallback for older PyTorch\n",
    "        return torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "def get_acts_paligemma(\n",
    "    model, device,\n",
    "    model_name=MODEL_NAME,\n",
    "    *, filenames: Optional[List[str]] = None, text: Optional[List[str]] = None,\n",
    "    batch_size=32, use_amp=True, mode=\"lm\", pad_to_max=None\n",
    "):\n",
    "    if (text is not None) and (filenames is not None):\n",
    "        raise ValueError(\"Provide either text or image, not both.\")\n",
    "\n",
    "    feats = []\n",
    "    model.eval()\n",
    "\n",
    "    # IMAGE branch\n",
    "    if filenames is not None:\n",
    "        proc = AutoProcessor.from_pretrained(model_name)\n",
    "        if mode == \"raw\":\n",
    "            model.vision_tower.config.output_hidden_states = True\n",
    "        else:\n",
    "            model.language_model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for i in range(0, len(filenames), batch_size):\n",
    "                fbatch = filenames[i:i+batch_size]\n",
    "                imgs = [Image.open(fp).convert(\"RGB\") for fp in fbatch]\n",
    "\n",
    "                if mode == \"raw\":\n",
    "                    enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\")\n",
    "                    px = enc[\"pixel_values\"].to(device, non_blocking=True)\n",
    "                    vout = model.vision_tower(pixel_values=px, output_hidden_states=True, return_dict=True)\n",
    "                    hs = vout.hidden_states  # tuple of layers: [B, seq, 2304]\n",
    "                    print(f\"hs.shape: {hs.shape}\")\n",
    "                else:\n",
    "                    ############################# direct pass to the full model\n",
    "                    enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\").to(device)\n",
    "                    out = model(\n",
    "                                **enc,\n",
    "                                output_hidden_states=True,\n",
    "                                return_dict=True\n",
    "                            )\n",
    "                    \n",
    "                    hs_tuple = out.hidden_states   # tuple(len = n_layers), each [B, seq, 2304]\n",
    "#                     print(f\"hs_tuple[0].shape: {hs_tuple[0].shape}\")\n",
    "                    ############################# manual pass to vision_tower -> proj_layer -> language_model (more VRAM demanding)\n",
    "#                     enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\")\n",
    "#                     px = enc[\"pixel_values\"].to(device, non_blocking=True)\n",
    "#                     vout = model.vision_tower(pixel_values=px, output_hidden_states=False, return_dict=False)[0]\n",
    "#                     del px\n",
    "#                     print(f\"vout.shape: {vout.shape}\")\n",
    "#                     proj=model.multi_modal_projector(vout)\n",
    "#                     del vout                 \n",
    "#   #                  print(f'shape of input_ids: {enc[\"input_ids\"].shape}, input_ids: {enc[\"input_ids\"]}')\n",
    "#                     tok_embeds = model.language_model.embed_tokens(enc[\"input_ids\"][...,256:].to(device, non_blocking=True))\n",
    "#                     print(f\"tok_embeds.shape: {tok_embeds.shape}\")\n",
    "#                     inputs_embeds = torch.cat([proj, tok_embeds], dim=1)\n",
    "#                     print(f\"inputs_embeds.shape: {inputs_embeds.shape}\")\n",
    "#                     lm_inputs = {\n",
    "#                         \"inputs_embeds\": inputs_embeds,             # vision embeddings\n",
    "#                         \"attention_mask\": enc[\"attention_mask\"],\n",
    "#                     }\n",
    "#                     del proj\n",
    "#                     out = model.language_model(**lm_inputs, output_hidden_states=True, return_dict=True)\n",
    "#                     hs_tuple = out.hidden_states\n",
    "#                     print(f\"hs_tuple[0].shape: {hs_tuple[0].shape}\")\n",
    "#                     del out\n",
    "\n",
    "                feats.append([h.detach().cpu().float().numpy() for h in hs_tuple])\n",
    "                del hs_tuple, enc, imgs\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # TEXT branch\n",
    "    elif text is not None:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        model.language_model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for i in range(0, len(text), batch_size):\n",
    "                tbatch = text[i:i+batch_size]\n",
    "                enc = tok(\n",
    "                    tbatch, return_tensors=\"pt\",\n",
    "                    padding=\"max_length\" if pad_to_max else True,\n",
    "                    truncation=True, max_length=pad_to_max\n",
    "                ).to(device)\n",
    "\n",
    "                out = model.language_model(**enc, output_hidden_states=True, return_dict=True)\n",
    "                hs = out.hidden_states  # tuple: [B, seq, 2304]\n",
    "\n",
    "                feats.append([h.detach().cpu().float().numpy() for h in hs])\n",
    "                del hs, enc, out\n",
    "                torch.cuda.empty_cache()\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either filenames or text.\")\n",
    "\n",
    "    # concatenate across batches per layer\n",
    "    n_layers = len(feats[0])\n",
    "    layerwise = []\n",
    "    for l in range(n_layers):\n",
    "        arrs = [batch[l] for batch in feats]    # list of [B, seq, D]\n",
    "        layerwise.append(np.concatenate(arrs, axis=0))  # [N, seq, D] (consistent seq if padded)\n",
    "\n",
    "    return layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e64ac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.089Z"
    }
   },
   "outputs": [],
   "source": [
    "# ====================== minimal steering helpers (NEW) ======================\n",
    "STEER_ALPHA = 1.0  # you can sweep this; positive increases predicted \"brightness\" if probe learned that.\n",
    "GEN_SAMPLE_IX = 0  # which test example to use for steered generation capture\n",
    "\n",
    "def _safe_unit(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x)\n",
    "    return x / (n + 1e-12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d043658",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.091Z"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Pairwise-Controlled Probing for Brightness\n",
    "# Split by base_id (group), extract activations for each variant, and run a linear probe per layer.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "STRENGTH = 1\n",
    "\n",
    "df = pd.read_csv(OUT_CSV)  # columns: base_id, variant_path, label, caption, ...\n",
    "\n",
    "#################################### computation saver ####################################\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Get unique base_ids\n",
    "# unique_base_ids = df['base_id'].unique()\n",
    "\n",
    "# # Sample 100 unique base_ids\n",
    "# np.random.seed(42)  # For reproducibility - remove if you want different results each time\n",
    "# sampled_base_ids = np.random.choice(unique_base_ids, size=100, replace=False)\n",
    "\n",
    "# # Filter the dataframe to include only rows with the sampled base_ids\n",
    "# sampled_df = df[df['base_id'].isin(sampled_base_ids)].copy()\n",
    "\n",
    "# # Sort by base_id to keep pairs together (optional but nice for viewing)\n",
    "# sampled_df = sampled_df.sort_values('base_id').reset_index(drop=True)\n",
    "\n",
    "# print(f\"Original dataset: {len(df)} rows with {len(unique_base_ids)} unique base_ids\")\n",
    "# print(f\"Sampled dataset: {len(sampled_df)} rows with {len(sampled_df['base_id'].unique())} unique base_ids\")\n",
    "# print(f\"Each base_id should have exactly 2 entries: {sampled_df['base_id'].value_counts().unique()}\")\n",
    "# df = sampled_df\n",
    "#################################### computation saver ####################################\n",
    "\n",
    "# group-wise split on base_id (prevents identity leakage)\n",
    "unique_ids = sorted(df['base_id'].unique())\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=SEED)\n",
    "\n",
    "df_tr = df[df['base_id'].isin(train_ids)].reset_index(drop=True)\n",
    "df_te = df[df['base_id'].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train groups: {len(train_ids)}, Test groups: {len(test_ids)}\")\n",
    "print(f\"Train rows: {len(df_tr)}, Test rows: {len(df_te)}\")\n",
    "\n",
    "# extract (images only for brightness), LM space (so D=1152), mean-pool tokens per sample\n",
    "with torch.inference_mode():\n",
    "    img_layers_tr = get_acts_paligemma(\n",
    "        model, device, filenames=df_tr['variant_path'].tolist(),\n",
    "        mode=MODE, pad_to_max=None\n",
    "    )\n",
    "    img_layers_te = get_acts_paligemma(\n",
    "        model, device, filenames=df_te['variant_path'].tolist(),\n",
    "        mode=MODE, pad_to_max=None\n",
    "    )\n",
    "\n",
    "n_layers = len(img_layers_tr)\n",
    "layer_ix = list(range(n_layers))\n",
    "\n",
    "all_rows = []\n",
    "for layer in layer_ix:\n",
    "    print(f\"layer:{layer}\")\n",
    "    X_tr = img_layers_tr[layer].mean(axis=1)  # [N_train, D]\n",
    "    y_tr = df_tr['label'].to_numpy()\n",
    "    X_te = img_layers_te[layer].mean(axis=1)  # [N_test, D]\n",
    "    y_te = df_te['label'].to_numpy()\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=SEED).fit(X_tr, y_tr)\n",
    "    yhat_tr = clf.predict(X_tr)\n",
    "    yhat_te = clf.predict(X_te)\n",
    "\n",
    "    tr_acc = accuracy_score(y_tr, yhat_tr)\n",
    "    te_acc = accuracy_score(y_te, yhat_te)\n",
    "    te_f1  = f1_score(y_te, yhat_te, average=\"macro\")\n",
    "    \n",
    "    ################################### steering with weights of the linear layer of the probe of this layer of PaliGemma ########################\n",
    "    W = torch.tensor(clf.coef_.ravel(), dtype=torch.float32) # 2304\n",
    "#     W = W.view(1, 1, -1) # (1, 1, d_hidden)\n",
    "    b = clf.intercept_.copy()\n",
    "#     print(W)\n",
    "#     print(W.shape)\n",
    "#     print(b)\n",
    "#     print(b.shape)\n",
    "    # Test steering on a neutral prompt\n",
    "    test_prompt = \"Answer in [bright, dark]. Sam looks out of the window - The sky is\"\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    orig_out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
    "    time_factors = [0, 1, 2, 5, 10, 20]\n",
    "#     steered_outs = {}\n",
    "    \n",
    "    for time_factor in time_factors:\n",
    "        steered_out = apply_steering(model, inputs, W, layer_idx=layer, strength=STRENGTH*time_factor)\n",
    "        steered_out = tokenizer.decode(steered_out[0], skip_special_tokens=True)\n",
    "        all_rows.append({f'steered_outs_{STRENGTH*time_factor}':steered_out})\n",
    "        print(f\"layer: {layer}, steered: {steered_out}, strength: {STRENGTH*time_factor}\")\n",
    "    \n",
    "#     hooks = [] # Where I want to hook, MLP, residual, etc.\n",
    "    ################################# steering with weights of the linear layer of the probe of this layer of PaliGemma ########################\n",
    "\n",
    "    print(f\"Layer {layer:2d} | Train {tr_acc:.4f} | Test {te_acc:.4f} | F1 {te_f1:.4f}\")\n",
    "    all_rows.append({\"layer\": layer, \"train_acc\": tr_acc, \"test_acc\": te_acc, \"test_f1\": te_f1})\n",
    "\n",
    "\n",
    "# Save results and plot\n",
    "res_df = pd.DataFrame(all_rows)\n",
    "res_csv = str(Path(OUTPUT_DIR) / \"results.csv\")\n",
    "res_plot = str(Path(OUTPUT_DIR) / \"accuracy_f1_curve.png\")\n",
    "res_df.to_csv(res_csv, index=False)\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(res_df[\"layer\"], res_df[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(res_df[\"layer\"], res_df[\"test_acc\"],  label=\"Test Acc\")\n",
    "plt.plot(res_df[\"layer\"], res_df[\"test_f1\"],   label=\"Test F1\", linestyle=\"--\", marker=\"o\")\n",
    "# highlight layer 0\n",
    "# if 0 in res_df[\"layer\"].values:\n",
    "#     i0 = res_df.index[res_df[\"layer\"]==0][0]\n",
    "#     plt.scatter([0], [res_df.loc[i0, \"test_acc\"]], s=60, edgecolors=\"k\", label=\"Layer 0 (Test Acc)\")\n",
    "plt.xlabel(\"Layer\"); plt.ylabel(\"Score\"); plt.title(\"Brightness Probe (Pairwise-Controlled)\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.savefig(res_plot, dpi=150); plt.close()\n",
    "\n",
    "print(f\"Saved results -> {res_csv}\\nSaved plot -> {res_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a458a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.092Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Make sure you're actually on PaliGemma 2 artifacts:\n",
    "print(model.__class__.__name__)                       # PaliGemmaForConditionalGeneration\n",
    "print(type(model.language_model).__name__)                # Gemma2ForCausalLM\n",
    "print(model.config.text_config.model_type)            # 'gemma2'\n",
    "\n",
    "# 2) Shapes that betray Gemma 2:\n",
    "print(model.text_model.model.embed_tokens.weight.shape)  # [~256k, 2304]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfa522",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weird enough, when using PaliGemmaForConditionalGeneration, the d_hidden is 2048 instead of 2504 for language_model\n",
    "# Because that's a Gemma instead of Gemma2! see model printdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254ee26",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.095Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(res_csv)\n",
    "res_df.steered_out_2 = res_df.steered_out_2.apply(lambda x:tokenizer.decode(eval(x)[0])).tolist()\n",
    "res_df.steered_out_4 = res_df.steered_out_4.apply(lambda x:tokenizer.decode(eval(x)[0])).tolist()\n",
    "# res_df['steered_out_2'].tolist(), res_df['steered_out_4'].tolist(), res_df['steered_out_2'] == res_df['steered_out_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b10d45",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.095Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58deea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.097Z"
    }
   },
   "outputs": [],
   "source": [
    "img_layers_tr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d72013",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-09T15:20:14.098Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79120b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "908ea243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "SAVE_DIR= '../data'\n",
    "data_imgs_text = pd.read_csv(f\"{SAVE_DIR}/coco_imgs_text_balanced_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d63dade4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modality</th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image</td>\n",
       "      <td>../data/val2017/000000217425.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>000000217425.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>The restored old time car is parked near a fou...</td>\n",
       "      <td>0</td>\n",
       "      <td>000000493286.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>A woman in a hat sitting next to luggage.</td>\n",
       "      <td>0</td>\n",
       "      <td>000000437351.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image</td>\n",
       "      <td>../data/val2017/000000188592.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>000000188592.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image</td>\n",
       "      <td>../data/val2017/000000515577.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>000000515577.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>text</td>\n",
       "      <td>a home made three wheeled trike sitting in a d...</td>\n",
       "      <td>0</td>\n",
       "      <td>000000007386.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>image</td>\n",
       "      <td>../data/val2017/000000125245.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>000000125245.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>text</td>\n",
       "      <td>A man has his face up to a video game monitor.</td>\n",
       "      <td>0</td>\n",
       "      <td>000000164883.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>text</td>\n",
       "      <td>some zebras leaning down to eat some grass</td>\n",
       "      <td>0</td>\n",
       "      <td>000000085823.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>text</td>\n",
       "      <td>a baseball player gets ready to lay down a bunt</td>\n",
       "      <td>0</td>\n",
       "      <td>000000485480.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   modality                                              input  label  \\\n",
       "0     image                   ../data/val2017/000000217425.jpg      1   \n",
       "1      text  The restored old time car is parked near a fou...      0   \n",
       "2      text          A woman in a hat sitting next to luggage.      0   \n",
       "3     image                   ../data/val2017/000000188592.jpg      1   \n",
       "4     image                   ../data/val2017/000000515577.jpg      1   \n",
       "..      ...                                                ...    ...   \n",
       "95     text  a home made three wheeled trike sitting in a d...      0   \n",
       "96    image                   ../data/val2017/000000125245.jpg      1   \n",
       "97     text     A man has his face up to a video game monitor.      0   \n",
       "98     text         some zebras leaning down to eat some grass      0   \n",
       "99     text   a baseball player gets ready to lay down a bunt       0   \n",
       "\n",
       "           file_name  \n",
       "0   000000217425.jpg  \n",
       "1   000000493286.jpg  \n",
       "2   000000437351.jpg  \n",
       "3   000000188592.jpg  \n",
       "4   000000515577.jpg  \n",
       "..               ...  \n",
       "95  000000007386.jpg  \n",
       "96  000000125245.jpg  \n",
       "97  000000164883.jpg  \n",
       "98  000000085823.jpg  \n",
       "99  000000485480.jpg  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_imgs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5e04f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 45\n"
     ]
    }
   ],
   "source": [
    "texts, text_labels  = data_imgs_text[data_imgs_text.modality == 'text'].input.tolist(), data_imgs_text[data_imgs_text.modality == 'text'].label.tolist() \n",
    "filenames, filenames_labels = data_imgs_text[data_imgs_text.modality == 'image'].file_name.tolist(), data_imgs_text[data_imgs_text.modality == 'image'].label.tolist()\n",
    "print(len(texts), len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33648b2",
   "metadata": {},
   "source": [
    "# probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c6e5f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PaliGemmaForConditionalGeneration\n",
    "from transformers import PaliGemmaForConditionalGeneration\n",
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "def seed_everywhere(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)   \n",
    "    \n",
    "seed_everywhere(42)\n",
    "device = \"cuda:1\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    \"google/paligemma2-3b-pt-224\", \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,  # Use fp16 for memory efficiency\n",
    "    device_map=None  # We'll handle device placement manually\n",
    ")\n",
    "model = model.to(device)\n",
    "language_model = model.language_model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/paligemma2-3b-pt-224\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "416509f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read imgs\n",
    "from contextlib import nullcontext\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "import concurrent.futures\n",
    "import os, io, concurrent.futures\n",
    "\n",
    "IMG_ROOT = \"../data/coco_imgs_text_cls_imgs\" \n",
    "\n",
    "def _read_bytes(path: str) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def _amp_ctx(device, use_amp=True):\n",
    "    \"\"\"Return a context manager for autocast if on CUDA, else a no-op.\"\"\"\n",
    "    if not use_amp:\n",
    "        return nullcontext()\n",
    "    is_cuda = torch.cuda.is_available() and (\n",
    "        str(device).startswith(\"cuda\") or getattr(getattr(device, \"type\", None), \"__str__\", lambda: \"\")() == \"cuda\"\n",
    "        or (hasattr(device, \"type\") and device.type == \"cuda\")\n",
    "    )\n",
    "    if not is_cuda:\n",
    "        return nullcontext()\n",
    "    # Prefer new API if available\n",
    "    try:\n",
    "        return torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "    except Exception:\n",
    "        # Fallback for older PyTorch\n",
    "        return torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "    \n",
    "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".webp\")\n",
    "\n",
    "def get_acts_paligemma(\n",
    "    model, device,\n",
    "    model_name='google/paligemma2-3b-pt-224',\n",
    "    *,\n",
    "    filenames=None, img_byte_cache=None, text=None,\n",
    "    batch_size=32, use_amp=True,\n",
    "    mode=\"lm\",  # \"raw\"=raw towers (2304 vs 1152), \"lm\"=language model space (1152 vs 1152)\n",
    "    # There is actually no use to get raw towers since that's not we are interested in\n",
    "    pad_to_max=258   # int: pad/truncate text to fixed length (e.g., 64, the same length as an image in gemma). None = variable length\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract hidden states from PaliGemma.\n",
    "    \n",
    "    Args:\n",
    "        model: the loaded PaliGemma model\n",
    "        device: GPU/CPU device\n",
    "        model_name: HF model name\n",
    "        filenames: list of image filenames (if using images)\n",
    "        img_byte_cache: dict {filename: bytes} for image loading\n",
    "        text: list of text strings (if using text)\n",
    "        batch_size: minibatch size\n",
    "        use_amp: whether to use autocast\n",
    "        mode: \"raw\" for raw tower outputs (2304 vs 1152),\n",
    "              \"lm\" for language-model space (1152 vs 1152)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray of hidden states concatenated across batches.\n",
    "        Shape:\n",
    "          - raw/image: [N, seq_len_img, 2304]\n",
    "          - raw/text:  [N, seq_len_txt, 1152]\n",
    "          - lm/image: [N, seq_len_img_proj, 1152]; seq_len_img_proj = 258 in the Gemma part in PaliGemma\n",
    "          - lm/text:  [N, seq_len_txt, 1152]\n",
    "    \"\"\"\n",
    "    if text and filenames: \n",
    "        raise ValueError(\"Provide either text or image, not both.\")\n",
    "    \n",
    "    feats = []\n",
    "    model.eval()\n",
    "    \n",
    "    # ---------------- IMAGE branch ----------------\n",
    "    if filenames is not None:\n",
    "        assert img_byte_cache is not None, \"Need img_byte_cache for image path mode\"\n",
    "        proc = AutoProcessor.from_pretrained(model_name)\n",
    "        if mode == \"raw\":\n",
    "            model.vision_tower.config.output_hidden_states = True\n",
    "        else:\n",
    "            model.language_model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for start in range(0, len(filenames), batch_size):\n",
    "                fnb = filenames[start:start+batch_size]\n",
    "                imgs = [Image.open(io.BytesIO(img_byte_cache[fn])).convert(\"RGB\") for fn in fnb]\n",
    "\n",
    "                if mode == \"raw\":\n",
    "                    enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\")\n",
    "                    px = enc[\"pixel_values\"].to(device, non_blocking=True)\n",
    "                    vout = model.vision_tower(pixel_values=px, output_hidden_states=True, return_dict=True)\n",
    "                    hs = vout.hidden_states  # tuple of (layers) [B, seq, 2304]\n",
    "                else:  # LM mode (image tokens projected → 1152)\n",
    "                    enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\").to(device)\n",
    "                    out = model.language_model(**enc, output_hidden_states=True, return_dict=True)\n",
    "                    hs = out.hidden_states  # tuple of (layers) [B, seq, 1152]\n",
    "\n",
    "                feats.append([h.detach().cpu().float().numpy() for h in hs])\n",
    "                del hs, enc, imgs\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # ---------------- TEXT branch ----------------\n",
    "    elif text is not None:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        model.language_model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for start in range(0, len(text), batch_size):\n",
    "                tbatch = text[start:start+batch_size]\n",
    "                enc = tok(\n",
    "                    tbatch,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\" if pad_to_max else True,\n",
    "                    truncation=True,\n",
    "                    max_length=pad_to_max\n",
    "                ).to(device)\n",
    "\n",
    "                out = model.language_model(**enc, output_hidden_states=True, return_dict=True)\n",
    "                hs = out.hidden_states  # tuple of (layers) [B, seq, 1152]\n",
    "\n",
    "                feats.append([h.detach().cpu().float().numpy() for h in hs])\n",
    "                del hs, enc, out\n",
    "                torch.cuda.empty_cache()\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either filenames or text.\")\n",
    "\n",
    "    # --- Reformat: concatenate batches along dim 0, preserve layers ---\n",
    "    n_layers = len(feats[0])\n",
    "    layerwise = []\n",
    "    for l in range(n_layers):\n",
    "        arrs = [batch[l] for batch in feats]\n",
    "        layerwise.append(np.concatenate(arrs, axis=0))  # [N, pad_to_max, D] if fixed, else [N, seq, D]\n",
    "\n",
    "    return layerwise\n",
    "\n",
    "def preload_image_bytes(filenames, root, max_workers: int = 12):\n",
    "    paths = [os.path.join(root, fn) for fn in filenames]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        data = list(ex.map(_read_bytes, paths))\n",
    "    return dict(zip(filenames, data))\n",
    "\n",
    "img_byte_cache = preload_image_bytes(filenames, IMG_ROOT, max_workers=12) if len(filenames) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6be757d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    img_acts = get_acts_paligemma(model, device = device, filenames=filenames, img_byte_cache=img_byte_cache)\n",
    "    txt_acts = get_acts_paligemma(model, device = device, text=texts, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74c947eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, (45, 258, 2304))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_acts), img_acts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "46df0f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, (55, 64, 2304))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_acts), txt_acts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "051ffca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3035324",
   "metadata": {},
   "source": [
    "## loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "layer_to_test = list(range(27)) # including the layer 0 which is the residual for embedding layer \n",
    "all_results = []\n",
    "\n",
    "# ---- Extract activations ----\n",
    "with torch.inference_mode():\n",
    "    img_layers = get_acts_paligemma(\n",
    "        model, device=device, filenames=filenames,\n",
    "        img_byte_cache=img_byte_cache, mode=\"lm\", pad_to_max=64\n",
    "    )\n",
    "    txt_layers = get_acts_paligemma(\n",
    "        model, device=device, text=texts, mode=\"lm\", pad_to_max=64\n",
    "    )\n",
    "        \n",
    "for layer in layer_to_test:\n",
    "    print(f\"\\n=== LAYER {layer} ===\")\n",
    "\n",
    "    # select this layer’s activations and pool\n",
    "    img_hs = img_layers[layer]   # [N_img, seq, D]\n",
    "    txt_hs = txt_layers[layer]   # [N_txt, seq, D]\n",
    "    img_vecs = img_hs.mean(axis=1)  # [N_img, D]\n",
    "    txt_vecs = txt_hs.mean(axis=1)  # [N_txt, D]\n",
    "\n",
    "    # ---- Prepare dataset ----\n",
    "    X = np.concatenate([txt_vecs, img_vecs], axis=0)\n",
    "    y = np.array([0]*len(txt_vecs) + [1]*len(img_vecs))\n",
    "\n",
    "    # ---- Train/test split ----\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "    y_pred_train, y_pred_test = clf.predict(X_train), clf.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test, average=\"macro\")\n",
    "    report = classification_report(y_test, y_pred_test, output_dict=False)\n",
    "\n",
    "    results = {\n",
    "        \"layer\": layer,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"report\": report\n",
    "    }\n",
    "    all_results.append(results)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1:       {test_f1:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "# ---- Save results ----\n",
    "os.makedirs(\"../figs_tabs/img_txt_cls\", exist_ok=True)\n",
    "df = pd.DataFrame([\n",
    "    {\"layer\": r[\"layer\"], \"train_acc\": r[\"train_acc\"],\n",
    "     \"test_acc\": r[\"test_acc\"], \"test_f1\": r[\"test_f1\"]}\n",
    "    for r in all_results\n",
    "])\n",
    "df.to_csv(\"../figs_tabs/img_txt_cls/results.csv\", index=False)\n",
    "\n",
    "# ---- Plot ----\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df[\"layer\"], df[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(df[\"layer\"], df[\"test_acc\"], label=\"Test Acc\")\n",
    "plt.plot(df[\"layer\"], df[\"test_f1\"], label=\"Test F1\", linestyle=\"--\", marker=\"o\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Linear Probe: Image vs Text Classification\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../figs_tabs/img_txt_cls/accuracy_f1_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99e73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f724acff",
   "metadata": {},
   "source": [
    "# LinearProbes_brightness\n",
    "\n",
    "Pairwise-controlled linear probing for **brightness** (−50% vs +50%) using COCO val2017 and PaliGemma.\n",
    "\n",
    "> Labels: 0 = darker (−50%), 1 = brighter (+50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dec379b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:32.563779Z",
     "start_time": "2025-09-05T14:02:29.960924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM allocated: 0.00 GB\n",
      "VRAM reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Delete specific tensors if you have them\n",
    "# del your_tensor_variable\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Check VRAM usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"VRAM reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b88339c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:32.577230Z",
     "start_time": "2025-09-05T14:02:32.566895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## 0. Config & Setup\n",
    "# Adjust paths as needed. This notebook uses COCO **val2017** only.\n",
    "\n",
    "import os, random, json, io\n",
    "from pathlib import Path\n",
    "\n",
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "SEED = 42\n",
    "def seed_everywhere(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)   \n",
    "seed_everywhere(SEED)\n",
    "COEFFICIENTS = '_[0.5, 1.5]'\n",
    "# --- Paths (edit these to match your local files) ---\n",
    "# ANNO_DIR = '../data/annotations_trainval2017/annotations'\n",
    "# IMG_DIR  = '../data/val2017'               # COCO val2017 images\n",
    "OUT_IMG_DIR = f'../data/brightness_pairs{COEFFICIENTS}'   # directory to save brightness-perturbed images\n",
    "OUT_CSV = f'../data/brightness_dataset{COEFFICIENTS}.csv' # CSV with variants & labels\n",
    "OUTPUT_DIR = f'../figs_tabs/brightness_probe_pairwise{COEFFICIENTS}'\n",
    "os.makedirs(OUT_IMG_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Probe params\n",
    "N_GROUPS = 200                # number of base images to sample (each yields 2 variants)\n",
    "PAD_TO_MAX = 64               # text max length when extracting LM activations\n",
    "MODE = \"lm\"                   # 'lm' 2304 or 'raw' 1152 (vision); we look at the representation difference in LM\n",
    "MODEL_NAME = 'google/paligemma2-3b-pt-224'\n",
    "\n",
    "print('Config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd63d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:34.967903Z",
     "start_time": "2025-09-05T14:02:32.578688Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing scikit-learn ...\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/py311/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Pillow ...\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/py311/lib/python3.11/site-packages (11.3.0)\n",
      "Environment ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## 1. Environment Check\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkg):\n",
    "    try:\n",
    "        __import__(pkg.split('==')[0].split('[')[0].replace('-', '_'))\n",
    "    except Exception:\n",
    "        print(f'Installing {pkg} ...')\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# Ensure deps (comment out if you manage env separately)\n",
    "for pkg in [\n",
    "    \"pycocotools\",\n",
    "    \"transformers>=4.41.0\",\n",
    "    \"torch\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"Pillow\",\n",
    "]:\n",
    "    try:\n",
    "        __import__(pkg.split('>=')[0].split('==')[0])\n",
    "    except Exception as e:\n",
    "        pip_install(pkg)\n",
    "\n",
    "print('Environment ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56282ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:43.905001Z",
     "start_time": "2025-09-05T14:02:34.970917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 20.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: google/paligemma-3b-pt-224\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, AutoProcessor\n",
    "from typing import List, Optional\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)\n",
    "\n",
    "# AMP context helper\n",
    "class amp_ctx:\n",
    "    def __init__(self, device='cuda', use_amp=True):\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp and (device == 'cuda')\n",
    "    def __enter__(self):\n",
    "        if self.use_amp:\n",
    "            self.ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)\n",
    "            self.ctx.__enter__()\n",
    "        else:\n",
    "            self.ctx = None\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if self.ctx is not None:\n",
    "            self.ctx.__exit__(exc_type, exc, tb)\n",
    "\n",
    "# Load model\n",
    "MODEL_NAME = \"google/paligemma-3b-pt-224\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "print('Model loaded:', MODEL_NAME)\n",
    "\n",
    "# --- MINIMAL STEERING CODE ---\n",
    "\n",
    "def get_steering_vector(model, tokenizer, pos_prompt: str, neg_prompt: str, layer_idx: int = 15):\n",
    "    \"\"\"Extract steering vector from positive/negative prompt pair\"\"\"\n",
    "    \n",
    "    # Tokenize prompts\n",
    "    pos_inputs = tokenizer(pos_prompt, return_tensors=\"pt\").to(device)\n",
    "    neg_inputs = tokenizer(neg_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Forward pass with hooks to capture activations\n",
    "    pos_acts = []\n",
    "    neg_acts = []\n",
    "    \n",
    "    def hook_fn(acts_list):\n",
    "        def hook(module, input, output):\n",
    "            acts_list.append(output[0].detach().clone())  # hidden states\n",
    "        return hook\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with amp_ctx(device):\n",
    "            # Get positive activations\n",
    "            handle = model.language_model.layers[layer_idx].register_forward_hook(hook_fn(pos_acts))\n",
    "            _ = model.language_model(**pos_inputs)\n",
    "            handle.remove()\n",
    "            \n",
    "            # Get negative activations  \n",
    "            handle = model.language_model.layers[layer_idx].register_forward_hook(hook_fn(neg_acts))\n",
    "            _ = model.language_model(**neg_inputs)\n",
    "            handle.remove()\n",
    "    \n",
    "    # Steering vector = difference in activations (at last token position)\n",
    "    pos_act = pos_acts[0][0, -1, :]  # [hidden_dim] at the last seq pos\n",
    "    neg_act = neg_acts[0][0, -1, :]  # [hidden_dim] \n",
    "    steering_vec = pos_act - neg_act\n",
    "    \n",
    "    return steering_vec\n",
    "\n",
    "def apply_steering(model, inputs, steering_vec: torch.Tensor, layer_idx: int = 15, strength: float = 1.0):\n",
    "    \"\"\"Apply steering vector during generation\"\"\"\n",
    "    device = inputs[\"input_ids\"].device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Ensure steering vec matches device & dtype\n",
    "    steering_vec = steering_vec.to(device=device, dtype=dtype)\n",
    "    def steering_hook(module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        # Add steering to last token position\n",
    "        hidden_states[0, -1, :] += strength * steering_vec # steer at the last seq pos\n",
    "        return (hidden_states,) + output[1:]\n",
    "    \n",
    "    # Register hook\n",
    "    handle = model.language_model.layers[layer_idx].register_forward_hook(steering_hook)\n",
    "    \n",
    "    try:\n",
    "        with amp_ctx(device):\n",
    "            outputs = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2342b752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:45.136156Z",
     "start_time": "2025-09-05T14:02:43.907978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original generation:\n",
      "Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark\n",
      "\n",
      "With positive steering (strength=2.0):\n",
      "Answer in [bright, dark]. Sam looks out of the window - The sky is bright\n",
      "\n",
      "With negative steering (strength=-2.0):\n",
      "Answer in [bright, dark]. Sam looks out of the window - The sky is\n"
     ]
    }
   ],
   "source": [
    "# Create steering vector for \"happy\" vs \"sad\" sentiment\n",
    "steering_vec = get_steering_vector(\n",
    "    model, tokenizer,\n",
    "    pos_prompt=\"A very bright picture of a cat sitting on a sofa\",\n",
    "    neg_prompt=\"A very dark picture of a cat sitting on a sofa\",\n",
    "    layer_idx=15\n",
    ")\n",
    "\n",
    "# Test steering on a neutral prompt\n",
    "test_prompt = \"Answer in [bright, dark]. Sam looks out of the window - The sky is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nOriginal generation:\")\n",
    "with torch.no_grad():\n",
    "    with amp_ctx(device):\n",
    "        orig_out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(orig_out[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nWith positive steering (strength=2.0):\")\n",
    "steered_out = apply_steering(model, inputs, steering_vec, layer_idx=15, strength=2)\n",
    "print(tokenizer.decode(steered_out[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nWith negative steering (strength=-2.0):\")\n",
    "steered_out = apply_steering(model, inputs, steering_vec, layer_idx=15, strength=-2)\n",
    "print(tokenizer.decode(steered_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504f6a8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:45.151479Z",
     "start_time": "2025-09-05T14:02:45.138269Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## 4. Activation Extraction (with pad_to_max + 'lm'/'raw' modes)\n",
    "\n",
    "# Returns list of arrays per layer: [N, seq_len, D]\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def _amp_ctx(device, use_amp=True):\n",
    "    \"\"\"Return a context manager for autocast if on CUDA, else a no-op.\"\"\"\n",
    "    if not use_amp:\n",
    "        return nullcontext()\n",
    "    is_cuda = torch.cuda.is_available() and (\n",
    "        str(device).startswith(\"cuda\") or getattr(getattr(device, \"type\", None), \"__str__\", lambda: \"\")() == \"cuda\"\n",
    "        or (hasattr(device, \"type\") and device.type == \"cuda\")\n",
    "    )\n",
    "    if not is_cuda:\n",
    "        return nullcontext()\n",
    "    # Prefer new API if available\n",
    "    try:\n",
    "        return torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "    except Exception:\n",
    "        # Fallback for older PyTorch\n",
    "        return torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "def get_acts_paligemma(\n",
    "    model, device,\n",
    "    model_name=MODEL_NAME,\n",
    "    *, filenames: Optional[List[str]] = None, text: Optional[List[str]] = None,\n",
    "    batch_size=32, use_amp=True, mode=\"lm\", pad_to_max=None\n",
    "):\n",
    "    if (text is not None) and (filenames is not None):\n",
    "        raise ValueError(\"Provide either text or image, not both.\")\n",
    "\n",
    "    feats = []\n",
    "    model.eval()\n",
    "\n",
    "    # IMAGE branch\n",
    "    if filenames is not None:\n",
    "        proc = AutoProcessor.from_pretrained(model_name)\n",
    "        if mode == \"raw\":\n",
    "            model.vision_tower.config.output_hidden_states = True\n",
    "        else:\n",
    "            model.language_model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for i in range(0, len(filenames), batch_size):\n",
    "                fbatch = filenames[i:i+batch_size]\n",
    "                imgs = [Image.open(fp).convert(\"RGB\") for fp in fbatch]\n",
    "\n",
    "                if mode == \"raw\":\n",
    "                    enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\")\n",
    "                    px = enc[\"pixel_values\"].to(device, non_blocking=True)\n",
    "                    vout = model.vision_tower(pixel_values=px, output_hidden_states=True, return_dict=True)\n",
    "                    hs = vout.hidden_states  # tuple of layers: [B, seq, 2304]\n",
    "                    print(f\"hs.shape: {hs.shape}\")\n",
    "                else:\n",
    "                    ############################# direct pass to the full model\n",
    "                    enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\").to(device)\n",
    "                    out = model(\n",
    "                                **enc,\n",
    "                                output_hidden_states=True,\n",
    "                                return_dict=True\n",
    "                            )\n",
    "                    \n",
    "                    hs_tuple = out.hidden_states   # tuple(len = n_layers), each [B, seq, 2304]\n",
    "#                     print(f\"hs_tuple[0].shape: {hs_tuple[0].shape}\")\n",
    "                    ############################# manual pass to vision_tower -> proj_layer -> language_model (more VRAM demanding)\n",
    "#                     enc = proc(images=imgs, text=[\"<image>\"]*len(imgs), return_tensors=\"pt\")\n",
    "#                     px = enc[\"pixel_values\"].to(device, non_blocking=True)\n",
    "#                     vout = model.vision_tower(pixel_values=px, output_hidden_states=False, return_dict=False)[0]\n",
    "#                     del px\n",
    "#                     print(f\"vout.shape: {vout.shape}\")\n",
    "#                     proj=model.multi_modal_projector(vout)\n",
    "#                     del vout                 \n",
    "#   #                  print(f'shape of input_ids: {enc[\"input_ids\"].shape}, input_ids: {enc[\"input_ids\"]}')\n",
    "#                     tok_embeds = model.language_model.embed_tokens(enc[\"input_ids\"][...,256:].to(device, non_blocking=True))\n",
    "#                     print(f\"tok_embeds.shape: {tok_embeds.shape}\")\n",
    "#                     inputs_embeds = torch.cat([proj, tok_embeds], dim=1)\n",
    "#                     print(f\"inputs_embeds.shape: {inputs_embeds.shape}\")\n",
    "#                     lm_inputs = {\n",
    "#                         \"inputs_embeds\": inputs_embeds,             # vision embeddings\n",
    "#                         \"attention_mask\": enc[\"attention_mask\"],\n",
    "#                     }\n",
    "#                     del proj\n",
    "#                     out = model.language_model(**lm_inputs, output_hidden_states=True, return_dict=True)\n",
    "#                     hs_tuple = out.hidden_states\n",
    "#                     print(f\"hs_tuple[0].shape: {hs_tuple[0].shape}\")\n",
    "#                     del out\n",
    "\n",
    "                feats.append([h.detach().cpu().float().numpy() for h in hs_tuple])\n",
    "                del hs_tuple, enc, imgs\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # TEXT branch\n",
    "    elif text is not None:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        model.language_model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.inference_mode(), _amp_ctx(device, use_amp):\n",
    "            for i in range(0, len(text), batch_size):\n",
    "                tbatch = text[i:i+batch_size]\n",
    "                enc = tok(\n",
    "                    tbatch, return_tensors=\"pt\",\n",
    "                    padding=\"max_length\" if pad_to_max else True,\n",
    "                    truncation=True, max_length=pad_to_max\n",
    "                ).to(device)\n",
    "\n",
    "                out = model.language_model(**enc, output_hidden_states=True, return_dict=True)\n",
    "                hs = out.hidden_states  # tuple: [B, seq, 2304]\n",
    "\n",
    "                feats.append([h.detach().cpu().float().numpy() for h in hs])\n",
    "                del hs, enc, out\n",
    "                torch.cuda.empty_cache()\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either filenames or text.\")\n",
    "\n",
    "    # concatenate across batches per layer\n",
    "    n_layers = len(feats[0])\n",
    "    layerwise = []\n",
    "    for l in range(n_layers):\n",
    "        arrs = [batch[l] for batch in feats]    # list of [B, seq, D]\n",
    "        layerwise.append(np.concatenate(arrs, axis=0))  # [N, seq, D] (consistent seq if padded)\n",
    "\n",
    "    return layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee24f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:02:45.157228Z",
     "start_time": "2025-09-05T14:02:45.152873Z"
    }
   },
   "outputs": [],
   "source": [
    "# ====================== minimal steering helpers (NEW) ======================\n",
    "STEER_ALPHA = 1.0  # you can sweep this; positive increases predicted \"brightness\" if probe learned that.\n",
    "GEN_SAMPLE_IX = 0  # which test example to use for steered generation capture\n",
    "\n",
    "def _safe_unit(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x)\n",
    "    return x / (n + 1e-12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47300de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.568057Z",
     "start_time": "2025-09-05T14:02:45.159266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train groups: 400, Test groups: 100\n",
      "Train rows: 800, Test rows: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0\n",
      "layer: 0, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark, strength: 0\n",
      "layer: 0, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 1\n",
      "layer: 0, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 2\n",
      "layer: 0, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 0, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 0, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is in with with for  download, strength: 20\n",
      "Layer  0 | Train 1.0000 | Test 0.9350 | F1 0.9350\n",
      "layer:1\n",
      "layer: 1, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 0\n",
      "layer: 1, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 1, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 2\n",
      "layer: 1, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 5\n",
      "layer: 1, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 1, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is my                                                               , strength: 20\n",
      "Layer  1 | Train 1.0000 | Test 0.9650 | F1 0.9650\n",
      "layer:2\n",
      "layer: 2, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 0\n",
      "layer: 2, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 2, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 2\n",
      "layer: 2, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 5\n",
      "layer: 2, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 2, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is books books, strength: 20\n",
      "Layer  2 | Train 1.0000 | Test 0.9550 | F1 0.9550\n",
      "layer:3\n",
      "layer: 3, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 0\n",
      "layer: 3, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 3, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 2\n",
      "layer: 3, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 3, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is red, strength: 10\n",
      "layer: 3, steered: Answer in [bright, dark]. Sam looks out of the window - The sky isto to toto ,   1  11 , to to to to to to                                           , strength: 20\n",
      "Layer  3 | Train 1.0000 | Test 0.9250 | F1 0.9250\n",
      "layer:4\n",
      "layer: 4, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 0\n",
      "layer: 4, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 4, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 2\n",
      "layer: 4, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark, strength: 5\n",
      "layer: 4, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 4, steered: Answer in [bright, dark]. Sam looks out of the window - The sky isdedesde fromsiyahsto fromadin fromdeda onche 11dededededededecheriecheriecheriecheriecheriecheriecheriecheriecheriecheriecheriecheriecherie from bleusindederoto lcheriecheriecherie fromcheriecheriecheriecherie at bleusdefromrotorotounaunacheriecherie, strength: 20\n",
      "Layer  4 | Train 1.0000 | Test 0.9450 | F1 0.9450\n",
      "layer:5\n",
      "layer: 5, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 0\n",
      "layer: 5, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 5, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 2\n",
      "layer: 5, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 5\n",
      "layer: 5, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark. Add another of our favorite fall of color light theatre, strength: 10\n",
      "layer: 5, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is + + holiday split split bri bri bri bri salu salu divided su divi divi divi dm dm dm dm dm dm dm dm dm dm dm dm dm dm dm dm dm  su mule mule dm dm dm dm dm dm dm dm * ** * * * salu salu salu salu salu salu salu salu salu salu salu salu, strength: 20\n",
      "Layer  5 | Train 1.0000 | Test 0.9050 | F1 0.9050\n",
      "layer:6\n",
      "layer: 6, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 0\n",
      "layer: 6, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 6, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 2\n",
      "layer: 6, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 6, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 6, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 20\n",
      "Layer  6 | Train 1.0000 | Test 0.9000 | F1 0.8999\n",
      "layer:7\n",
      "layer: 7, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 0\n",
      "layer: 7, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 1\n",
      "layer: 7, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 2\n",
      "layer: 7, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 5\n",
      "layer: 7, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 10\n",
      "layer: 7, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is nots : : : : : : : on in : order of first first in first!: : top of of ris of first of top of top oftop: : romantik : : top : : top : top : : first : : separ separ : : : : : : : : : : best, strength: 20\n",
      "Layer  7 | Train 1.0000 | Test 0.9400 | F1 0.9400\n",
      "layer:8\n",
      "layer: 8, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark, strength: 0\n",
      "layer: 8, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 8, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 2\n",
      "layer: 8, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 8, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 8, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 20\n",
      "Layer  8 | Train 1.0000 | Test 0.9250 | F1 0.9250\n",
      "layer:9\n",
      "layer: 9, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 0\n",
      "layer: 9, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 9, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 2\n",
      "layer: 9, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 5\n",
      "layer: 9, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 10\n",
      "layer: 9, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 20\n",
      "Layer  9 | Train 1.0000 | Test 0.9350 | F1 0.9350\n",
      "layer:10\n",
      "layer: 10, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, but, strength: 0\n",
      "layer: 10, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 10, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 2\n",
      "layer: 10, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 10, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 10, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and, strength: 20\n",
      "Layer 10 | Train 1.0000 | Test 0.9300 | F1 0.9300\n",
      "layer:11\n",
      "layer: 11, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 0\n",
      "layer: 11, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 11, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 2\n",
      "layer: 11, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 11, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 10\n",
      "layer: 11, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark dream, strength: 20\n",
      "Layer 11 | Train 1.0000 | Test 0.9050 | F1 0.9050\n",
      "layer:12\n",
      "layer: 12, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark, strength: 0\n",
      "layer: 12, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is dark, strength: 1\n",
      "layer: 12, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 2\n",
      "layer: 12, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 12, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 12, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and he dreams, strength: 20\n",
      "Layer 12 | Train 1.0000 | Test 0.8900 | F1 0.8900\n",
      "layer:13\n",
      "layer: 13, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 0\n",
      "layer: 13, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 1\n",
      "layer: 13, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 2\n",
      "layer: 13, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 5\n",
      "layer: 13, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 10\n",
      "layer: 13, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright dark, strength: 20\n",
      "Layer 13 | Train 1.0000 | Test 0.8200 | F1 0.8199\n",
      "layer:14\n",
      "layer: 14, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 0\n",
      "layer: 14, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 1\n",
      "layer: 14, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 2\n",
      "layer: 14, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 14, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 14, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, dark, strength: 20\n",
      "Layer 14 | Train 1.0000 | Test 0.8150 | F1 0.8150\n",
      "layer:15\n",
      "layer: 15, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 0\n",
      "layer: 15, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 15, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 2\n",
      "layer: 15, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 5\n",
      "layer: 15, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 15, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 20\n",
      "Layer 15 | Train 1.0000 | Test 0.8250 | F1 0.8249\n",
      "layer:16\n",
      "layer: 16, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 0\n",
      "layer: 16, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 16, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 2\n",
      "layer: 16, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 5\n",
      "layer: 16, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 10\n",
      "layer: 16, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright and dark, strength: 20\n",
      "Layer 16 | Train 1.0000 | Test 0.8000 | F1 0.7999\n",
      "layer:17\n",
      "layer: 17, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 0\n",
      "layer: 17, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 1\n",
      "layer: 17, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 2\n",
      "layer: 17, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 5\n",
      "layer: 17, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is, strength: 10\n",
      "layer: 17, steered: Answer in [bright, dark]. Sam looks out of the window - The sky is bright, strength: 20\n",
      "Layer 17 | Train 1.0000 | Test 0.8050 | F1 0.8050\n",
      "layer:18\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m#     steered_outs = {}\u001b[39;00m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m time_factor \u001b[38;5;129;01min\u001b[39;00m time_factors:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         steered_out = apply_steering(model, inputs, W, layer_idx=layer, strength=STRENGTH*time_factor)\n\u001b[32m     95\u001b[39m         steered_out = tokenizer.decode(steered_out[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     96\u001b[39m         all_rows.append({\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msteered_outs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTRENGTH*time_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m:steered_out})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mapply_steering\u001b[39m\u001b[34m(model, inputs, steering_vec, layer_idx, strength)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (hidden_states,) + output[\u001b[32m1\u001b[39m:]\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Register hook\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m handle = model.language_model.layers[layer_idx].register_forward_hook(steering_hook)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m amp_ctx(device):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py:334\u001b[39m, in \u001b[36mModuleList.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._modules.values())[idx])\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules[\u001b[38;5;28mself\u001b[39m._get_abs_string_index(idx)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py:316\u001b[39m, in \u001b[36mModuleList._get_abs_string_index\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    314\u001b[39m idx = operator.index(idx)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (-\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) <= idx < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of range\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx < \u001b[32m0\u001b[39m:\n\u001b[32m    318\u001b[39m     idx += \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: index 18 is out of range"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Pairwise-Controlled Probing for Brightness\n",
    "# Split by base_id (group), extract activations for each variant, and run a linear probe per layer.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "STRENGTH = 1\n",
    "\n",
    "df = pd.read_csv(OUT_CSV)  # columns: base_id, variant_path, label, caption, ...\n",
    "\n",
    "#################################### computation saver ####################################\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Get unique base_ids\n",
    "# unique_base_ids = df['base_id'].unique()\n",
    "\n",
    "# # Sample 100 unique base_ids\n",
    "# np.random.seed(42)  # For reproducibility - remove if you want different results each time\n",
    "# sampled_base_ids = np.random.choice(unique_base_ids, size=100, replace=False)\n",
    "\n",
    "# # Filter the dataframe to include only rows with the sampled base_ids\n",
    "# sampled_df = df[df['base_id'].isin(sampled_base_ids)].copy()\n",
    "\n",
    "# # Sort by base_id to keep pairs together (optional but nice for viewing)\n",
    "# sampled_df = sampled_df.sort_values('base_id').reset_index(drop=True)\n",
    "\n",
    "# print(f\"Original dataset: {len(df)} rows with {len(unique_base_ids)} unique base_ids\")\n",
    "# print(f\"Sampled dataset: {len(sampled_df)} rows with {len(sampled_df['base_id'].unique())} unique base_ids\")\n",
    "# print(f\"Each base_id should have exactly 2 entries: {sampled_df['base_id'].value_counts().unique()}\")\n",
    "# df = sampled_df\n",
    "#################################### computation saver ####################################\n",
    "\n",
    "# group-wise split on base_id (prevents identity leakage)\n",
    "unique_ids = sorted(df['base_id'].unique())\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=SEED)\n",
    "\n",
    "df_tr = df[df['base_id'].isin(train_ids)].reset_index(drop=True)\n",
    "df_te = df[df['base_id'].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train groups: {len(train_ids)}, Test groups: {len(test_ids)}\")\n",
    "print(f\"Train rows: {len(df_tr)}, Test rows: {len(df_te)}\")\n",
    "\n",
    "# extract (images only for brightness), LM space (so D=1152), mean-pool tokens per sample\n",
    "with torch.inference_mode():\n",
    "    img_layers_tr = get_acts_paligemma(\n",
    "        model, device, filenames=df_tr['variant_path'].tolist(),\n",
    "        mode=MODE, pad_to_max=None\n",
    "    )\n",
    "    img_layers_te = get_acts_paligemma(\n",
    "        model, device, filenames=df_te['variant_path'].tolist(),\n",
    "        mode=MODE, pad_to_max=None\n",
    "    )\n",
    "\n",
    "n_layers = len(img_layers_tr)\n",
    "layer_ix = list(range(n_layers))\n",
    "\n",
    "all_rows = []\n",
    "for layer in layer_ix:\n",
    "    print(f\"layer:{layer}\")\n",
    "    X_tr = img_layers_tr[layer].mean(axis=1)  # [N_train, D]\n",
    "    y_tr = df_tr['label'].to_numpy()\n",
    "    X_te = img_layers_te[layer].mean(axis=1)  # [N_test, D]\n",
    "    y_te = df_te['label'].to_numpy()\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=SEED).fit(X_tr, y_tr)\n",
    "    yhat_tr = clf.predict(X_tr)\n",
    "    yhat_te = clf.predict(X_te)\n",
    "\n",
    "    tr_acc = accuracy_score(y_tr, yhat_tr)\n",
    "    te_acc = accuracy_score(y_te, yhat_te)\n",
    "    te_f1  = f1_score(y_te, yhat_te, average=\"macro\")\n",
    "    \n",
    "    ################################### steering with weights of the linear layer of the probe of this layer of PaliGemma ########################\n",
    "    W = torch.tensor(clf.coef_.ravel(), dtype=torch.float32) # 2304\n",
    "#     W = W.view(1, 1, -1) # (1, 1, d_hidden)\n",
    "    b = clf.intercept_.copy()\n",
    "#     print(W)\n",
    "#     print(W.shape)\n",
    "#     print(b)\n",
    "#     print(b.shape)\n",
    "    # Test steering on a neutral prompt\n",
    "    test_prompt = \"Answer in [bright, dark]. Sam looks out of the window - The sky is\"\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    orig_out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
    "    time_factors = [0, 1, 2, 5, 10, 20]\n",
    "#     steered_outs = {}\n",
    "    \n",
    "    for time_factor in time_factors:\n",
    "        steered_out = apply_steering(model, inputs, W, layer_idx=layer, strength=STRENGTH*time_factor)\n",
    "        steered_out = tokenizer.decode(steered_out[0], skip_special_tokens=True)\n",
    "        all_rows.append({f'steered_outs_{STRENGTH*time_factor}':steered_out})\n",
    "        print(f\"layer: {layer}, steered: {steered_out}, strength: {STRENGTH*time_factor}\")\n",
    "    \n",
    "#     hooks = [] # Where I want to hook, MLP, residual, etc.\n",
    "    ################################# steering with weights of the linear layer of the probe of this layer of PaliGemma ########################\n",
    "\n",
    "    print(f\"Layer {layer:2d} | Train {tr_acc:.4f} | Test {te_acc:.4f} | F1 {te_f1:.4f}\")\n",
    "    all_rows.append({\"layer\": layer, \"train_acc\": tr_acc, \"test_acc\": te_acc, \"test_f1\": te_f1})\n",
    "\n",
    "\n",
    "# Save results and plot\n",
    "res_df = pd.DataFrame(all_rows)\n",
    "res_csv = str(Path(OUTPUT_DIR) / \"results.csv\")\n",
    "res_plot = str(Path(OUTPUT_DIR) / \"accuracy_f1_curve.png\")\n",
    "res_df.to_csv(res_csv, index=False)\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(res_df[\"layer\"], res_df[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(res_df[\"layer\"], res_df[\"test_acc\"],  label=\"Test Acc\")\n",
    "plt.plot(res_df[\"layer\"], res_df[\"test_f1\"],   label=\"Test F1\", linestyle=\"--\", marker=\"o\")\n",
    "# highlight layer 0\n",
    "# if 0 in res_df[\"layer\"].values:\n",
    "#     i0 = res_df.index[res_df[\"layer\"]==0][0]\n",
    "#     plt.scatter([0], [res_df.loc[i0, \"test_acc\"]], s=60, edgecolors=\"k\", label=\"Layer 0 (Test Acc)\")\n",
    "plt.xlabel(\"Layer\"); plt.ylabel(\"Score\"); plt.title(\"Brightness Probe (Pairwise-Controlled)\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.savefig(res_plot, dpi=150); plt.close()\n",
    "\n",
    "print(f\"Saved results -> {res_csv}\\nSaved plot -> {res_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a2c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.576104Z",
     "start_time": "2025-09-05T14:04:56.576095Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Make sure you're actually on PaliGemma 2 artifacts:\n",
    "print(model.__class__.__name__)                       # PaliGemmaForConditionalGeneration\n",
    "print(type(model.language_model).__name__)                # Gemma2ForCausalLM\n",
    "print(model.config.text_config.model_type)            # 'gemma2'\n",
    "\n",
    "# 2) Shapes that betray Gemma 2:\n",
    "print(model.text_model.model.embed_tokens.weight.shape)  # [~256k, 2304]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf41ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.577135Z",
     "start_time": "2025-09-05T14:04:56.577127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weird enough, when using PaliGemmaForConditionalGeneration, the d_hidden is 2048 instead of 2504 for language_model\n",
    "# Because that's a Gemma instead of Gemma2! see model printdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810d762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.577650Z",
     "start_time": "2025-09-05T14:04:56.577642Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(res_csv)\n",
    "res_df.steered_out_2 = res_df.steered_out_2.apply(lambda x:tokenizer.decode(eval(x)[0])).tolist()\n",
    "res_df.steered_out_4 = res_df.steered_out_4.apply(lambda x:tokenizer.decode(eval(x)[0])).tolist()\n",
    "# res_df['steered_out_2'].tolist(), res_df['steered_out_4'].tolist(), res_df['steered_out_2'] == res_df['steered_out_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b359477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.578147Z",
     "start_time": "2025-09-05T14:04:56.578139Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed5c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.578793Z",
     "start_time": "2025-09-05T14:04:56.578785Z"
    }
   },
   "outputs": [],
   "source": [
    "img_layers_tr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb3c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T14:04:56.579905Z",
     "start_time": "2025-09-05T14:04:56.579897Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1beba23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
